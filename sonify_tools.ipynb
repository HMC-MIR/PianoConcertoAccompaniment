{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2491659a",
   "metadata": {},
   "source": [
    "# Sonification Tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fefb633d",
   "metadata": {},
   "source": [
    "This notebook implements a number of functions that are useful in generating sonifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f61739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from tsm_tools.ipynb\n",
      "importing Jupyter notebook from system_utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa as lb\n",
    "import soundfile as sf\n",
    "import os.path\n",
    "import import_ipynb\n",
    "import tsm_tools\n",
    "import system_utils\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "821d5723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_wp(align_file, downsample, hop_len = None):\n",
    "    '''\n",
    "    Converts the warping path to seconds (if expressed in frames) and downsamples.\n",
    "    \n",
    "    Inputs\n",
    "    align_file: filepath to the .npy file specifying the warping path between the two audio files\n",
    "                If hop_len is specified, the warping path is assumed to be expressed in frames.\n",
    "                If hop_len is not specified, the warping path is assumed to be expressed in seconds.\n",
    "    downsample: downsample the warping path by this factor to smooth out the TSM\n",
    "    hop_len: specifies the hop length in seconds between frames.  If not specified, the warping path\n",
    "             is assumed to be expressed in seconds.\n",
    "    \n",
    "    Returns the downsampled warping path expressed in seconds.\n",
    "    '''\n",
    "    if hop_len is None: \n",
    "        wp = np.load(align_file) # 2xN array specifying file1-file2 alignment in sec\n",
    "    else:\n",
    "        wp = np.load(align_file) # 2xN array specifying file1-file2 alignment in frames\n",
    "        wp = wp * hop_len # convert to sec    \n",
    "    wp_middle = wp[:,1:-1] # keep ends, downsample the middle\n",
    "    wp = np.hstack((wp[:,0].reshape((2,-1)), wp_middle[:,0::downsample], wp[:,-1].reshape((2,-1))))\n",
    "    return wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8cf97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_separate_channels(left_channel, right_channel, reweighted = True, pad = False):\n",
    "    '''\n",
    "    Merges two mono audio waveforms into a stereo audio waveform.  If the two waveforms differ\n",
    "    in length, the longer of the two is truncated unless `pad = True`.  Provides channel volume reweighting by default.\n",
    "    \n",
    "    Inputs\n",
    "    left_channel: the audio waveform for the left channel\n",
    "    right_channel: the audio waveform for the right channel\n",
    "    reweighted: if True, reweights the channels to have equal volume\n",
    "    pad: if True, pads the shorter waveform with zeros to match the length of the longer waveform\n",
    "    \n",
    "    Returns an Nx2 array containing the mixed stereo audio waveform.\n",
    "    '''\n",
    "    if pad:\n",
    "        if len(left_channel) > len(right_channel):\n",
    "            right_channel = np.pad(right_channel, (0, len(left_channel) - len(right_channel)))\n",
    "        elif len(right_channel) > len(left_channel):\n",
    "            left_channel = np.pad(left_channel, (0, len(right_channel) - len(left_channel)))\n",
    "        assert len(left_channel) == len(right_channel), \"Channels are not the same length\"\n",
    "    \n",
    "    N = min(len(left_channel), len(right_channel))\n",
    "    mixed = np.zeros((N, 2))\n",
    "    mixed[:,0] = left_channel[0:N]\n",
    "    mixed[:,1] = right_channel[0:N]\n",
    "    if reweighted:\n",
    "        mixed = channel_volume_reweighting(mixed)\n",
    "    return mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40902267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_volume_reweighting(x_stereo):\n",
    "    '''\n",
    "    Reweights the left and right channels to be approximately equal volume.\n",
    "    \n",
    "    Inputs\n",
    "    x_stereo: an Nx2 array containing the stereo audio waveform\n",
    "    \n",
    "    Returns an Nx2 array with the two audio channels reweighted in volume.\n",
    "    '''\n",
    "    mse_left = np.mean(x_stereo[:,0] * x_stereo[:,0])\n",
    "    mse_right = np.mean(x_stereo[:,1] * x_stereo[:,1])\n",
    "    x_stereo[:,1] = x_stereo[:,1] * np.sqrt(mse_left / mse_right)\n",
    "    return x_stereo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf19070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sonifyWithTSMSync(audiofile1, audiofile2, align_file, downsample, hop_len = None, outfile = None):\n",
    "    '''\n",
    "    Generates a stereo audio recording with one audio recording on the left channel and another audio\n",
    "    recording on the other channel, where time-scale modification has been applied to the latter so\n",
    "    that the two recordings are appropriately synchronized.  If the alignment is a subsequence alignment,\n",
    "    the shorter query recording should be specified as audiofile1.\n",
    "    \n",
    "    Inputs\n",
    "    audiofile1: filepath to the audio recording that will remain unmodified\n",
    "    audiofile2: filepath to the audio recording that will be time-scaled modified\n",
    "    align_file: filepath to the .npy file specifying the warping path between the two audio files.\n",
    "                If hop_len is specified, the warping path is assumed to be expressed in frames.\n",
    "                If hop_len is not specified, the warping path is assumed to be expressed in seconds.\n",
    "    downsample: downsample the warping path by this factor to smooth out the TSM\n",
    "    hop_len: specifies the hop length in seconds between frames.  If this is not specified, it will\n",
    "             be assumed that the warping path is already expressed in seconds.\n",
    "    outfile: the output audio file to generate\n",
    "    '''\n",
    "    y1, sr = lb.load(audiofile1)\n",
    "    y2, sr = lb.load(audiofile2)\n",
    "    if len(y1) > len(y2):\n",
    "        print('Warning: If synchronization uses a subsequence alignment, the shorter query should be specified as audiofile1')\n",
    "    wp = get_preprocessed_wp(align_file, downsample, hop_len) # file1-file2 alignment\n",
    "    wp = system_utils.filter_vertical_and_horizontal_segments(wp)\n",
    "    y2_tsm = tsm_tools.tsmvar_hybrid(y2, np.flipud(wp))\n",
    "    y_mixed = mix_separate_channels(y1, y2_tsm)\n",
    "    if outfile:\n",
    "        sf.write(outfile, y_mixed, sr, subtype='PCM_16')\n",
    "    return y_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0169df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sonifyWithTSMSync_batch(scenarios_dir, exp_dir, downsample, hop_len, outdir):\n",
    "    '''\n",
    "    Generates stereo recordings of piano (left channel) and time-scale modified orchestra recordings\n",
    "    (right channel) for all scenarios.\n",
    "    \n",
    "    Inputs\n",
    "    scenarios_dir: directory containing all scenario directories\n",
    "    exp_dir: directory containing all the hypothesis alignments\n",
    "    downsample: downsample the warping path by this factor to smooth out the TSM\n",
    "    hop_len: specifies the hop length in seconds between frames, needed to convert warping path to timestamps\n",
    "    outdir: directory to put the generated audio files\n",
    "    '''\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    \n",
    "    summary_file = f'{scenarios_dir}/scenarios.summary'\n",
    "    scenario_ids = system_utils.get_scenario_info(summary_file).keys()\n",
    "    for scenario_id in tqdm(scenario_ids):\n",
    "        piano_file = f'{scenarios_dir}/{scenario_id}/p.wav'\n",
    "        orch_file = f'{scenarios_dir}/{scenario_id}/o.wav'\n",
    "        align_file = f'{exp_dir}/{scenario_id}/hyp.npy'\n",
    "        out_file = f'{outdir}/{scenario_id}.wav'\n",
    "        if os.path.exists(out_file):\n",
    "            print(f'Skipping sonification of {scenario_id} -- already exists')\n",
    "            continue\n",
    "        else:\n",
    "            # print(f'Generating sonification of {scenario_id}')\n",
    "            sonifyWithTSMSync(piano_file, orch_file, align_file, downsample, hop_len, out_file)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27627aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
