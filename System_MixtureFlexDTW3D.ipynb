{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6a79b99",
   "metadata": {},
   "source": [
    "# Offline FlexDTW 3D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cb97f15",
   "metadata": {},
   "source": [
    "This notebook provides wrapper functions for calling the [ISA (Iterative Subtractive Alignment) algorithm](https://archives.ismir.net/ismir2021/paper/000101.pdf).  Running this algorithm requires installing some other software, which is described below.  This notebook implements the `offline_processing()` and `online_processing()` functions, which will be imported and run in `02_RunExperiment.ipynb`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70d1df11",
   "metadata": {},
   "source": [
    "## Offline Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babb328e",
   "metadata": {},
   "source": [
    "The offline processing is the same as in the simple offline DTW system.  In the offline processing stage, three things are computed and stored in the `cache/` folder:\n",
    "- chroma features for the orchestra recording\n",
    "- chroma features for the full mix recording\n",
    "- predicted DTW alignment between the orchestra and full mix recordings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0653f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from align_tools.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "import system_utils\n",
    "import align_tools\n",
    "import sonify_tools\n",
    "import os\n",
    "import os.path\n",
    "import subprocess\n",
    "import librosa as lb\n",
    "from shutil import which\n",
    "from hmc_mir.align import isa\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit, prange\n",
    "import k3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e6185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_processing(scenario_dir, cache_dir, hop_length=2048, downsample=10):\n",
    "    '''\n",
    "    Carries out the same offline processing steps as the simple offline DTW system.\n",
    "    \n",
    "    Args:\n",
    "        scenario_dir: The scenario directory to process\n",
    "        cache_dir: The location of the cache directory\n",
    "        hop_length: The hop length in samples used when computing chroma features\n",
    "        downsample: The downsampling factor used when computing chroma_sens features\n",
    "    \n",
    "    This function will store the computed chroma features and estimated alignment in the cache folder.\n",
    "    '''\n",
    "    # setup\n",
    "    system_utils.verify_scenario_dir(scenario_dir)\n",
    "    if os.path.exists(cache_dir):\n",
    "        # print(f'{cache_dir} has already been processed.  Skipping.')\n",
    "        pass\n",
    "    else:\n",
    "        # setup\n",
    "        os.makedirs(cache_dir)\n",
    "        scenario_info = system_utils.get_scenario_info(scenario_dir+'/scenario.info')\n",
    "\n",
    "        o_file = f'{scenario_dir}/o.wav'\n",
    "        y_o, sr = lb.core.load(o_file)\n",
    "        \n",
    "        orch_start_frm = int(np.round(scenario_info['oStart'] * sr / hop_length))\n",
    "        orch_end_frm = int(np.round(scenario_info['oEnd'] * sr / hop_length)) + 1\n",
    "\n",
    "        F_o = lb.feature.chroma_cens(y=y_o, sr=sr, hop_length=hop_length)\n",
    "        F_o = F_o[:,orch_start_frm:orch_end_frm]\n",
    "        F_o = F_o[:,::downsample]\n",
    "\n",
    "        po_file = f'{scenario_dir}/po.wav'\n",
    "        y_po, sr = lb.core.load(po_file)\n",
    "        F_po = lb.feature.chroma_cens(y=y_po, sr=sr, hop_length=hop_length)\n",
    "        F_po = F_po[:,orch_start_frm:orch_end_frm]\n",
    "        F_po = F_po[:,::downsample]\n",
    "\n",
    "        np.save(f'{cache_dir}/po_cqt.npy', F_po)\n",
    "        np.save(f'{cache_dir}/o_cqt.npy', F_o)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c75fcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_cache_dir(indir):\n",
    "    '''\n",
    "    Verifies that the specified cache directory has the required files.\n",
    "    \n",
    "    Inputs\n",
    "    indir: The cache directory to verify\n",
    "    '''\n",
    "    assert os.path.exists(f'{indir}/po_cqt.npy'), f'missing {indir}/po_cqt.npy'\n",
    "    assert os.path.exists(f'{indir}/o_cqt.npy'), f'missing {indir}/o_cqt.npy'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "588ee8df",
   "metadata": {},
   "source": [
    "## Online Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d865c60",
   "metadata": {},
   "source": [
    "### Wrapper Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f22bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_processing(\n",
    "    scenario_dir,\n",
    "    out_dir,\n",
    "    cache_dir,\n",
    "    hop_length=2048,\n",
    "    downsample=10,\n",
    "    steps=np.array([\n",
    "        [1,1,1],\n",
    "        [2,1,1],\n",
    "        [1,2,1],\n",
    "        [1,1,2],\n",
    "        [2,2,1],\n",
    "        [2,1,2],\n",
    "        [1,2,2],\n",
    "    ]),\n",
    "    weights=np.array([3,4,4,4,5,5,5]),\n",
    "    buffer = 50\n",
    "):\n",
    "    '''\n",
    "    Carries out `online' processing using the ISA algorithm.\n",
    "    \n",
    "    Args:\n",
    "        scenario_dir: The scenario directory to process\n",
    "        out_dir: The directory to put results, intermediate files, and logging info\n",
    "        cache_dir: The cache directory\n",
    "        hop_length: The hop length in samples used when computing cqt features\n",
    "        downsample: The downsampling factor used when computing chroma_sens features\n",
    "\n",
    "    This function will compute and save the predicted alignment in the output directory in a file hyp.npy\n",
    "    '''\n",
    "    \n",
    "    # verify & setup\n",
    "    system_utils.verify_scenario_dir(scenario_dir)\n",
    "    verify_cache_dir(cache_dir)\n",
    "    assert not os.path.exists(out_dir), f'Output directory {out_dir} already exists.'\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "    scenario_info = system_utils.get_scenario_info(scenario_dir+'/scenario.info')\n",
    "    \n",
    "    p_file = f'{scenario_dir}/p.wav'\n",
    "    y, sr = lb.core.load(p_file)\n",
    "\n",
    "    F_p = lb.feature.chroma_cens(y=y, sr=sr, hop_length=hop_length)[:,::downsample]\n",
    "    F_po = np.load(f'{cache_dir}/po_cqt.npy') # full mix features\n",
    "    F_o = np.load(f'{cache_dir}/o_cqt.npy') # orchestra\n",
    "\n",
    "    hop_sec = downsample * hop_length / sr\n",
    "\n",
    "    # compute the alignment\n",
    "    best_cost, path = flexdtw(F_p, F_o, F_po, steps, weights, buffer=buffer)\n",
    "\n",
    "    wp_AC = np.vstack((path[0],path[1]))\n",
    "    wp_AC_sec = wp_AC*hop_sec\n",
    "    wp_AC_sec[1,:] += scenario_info['oStart']\n",
    "    np.save(f'{out_dir}/hyp.npy', wp_AC_sec)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b46c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_hyp_dir(indir):\n",
    "    '''\n",
    "    Verifies that the specified scenario hypothesis directory has the required files.\n",
    "    \n",
    "    Inputs\n",
    "    indir: The cache directory to verify\n",
    "    '''\n",
    "    assert os.path.exists(f'{indir}/hyp.npy'), f'{indir} is missing the required files, please re run the online processing'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b28c8bb",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cd9f6c3",
   "metadata": {},
   "source": [
    "Here is an example of how to call the offline and online processing functions on a scenario directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51aef5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario_dir = 'scenarios/s105'\n",
    "# out_dir = 'experiments/flexDTW/s105'\n",
    "# cache_dir = 'experiments/flexDTW/cache/beeth1_mov1_O1_PO1'\n",
    "# hop_length = 2048\n",
    "# downsample = 10\n",
    "# scenario_info = system_utils.get_scenario_info(scenario_dir+'/scenario.info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d17349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orch_start_frm = int(np.round(scenario_info['oStart'] * sr / hop_length))\n",
    "# orch_end_frm = int(np.round(scenario_info['oEnd'] * sr / hop_length)) + 1\n",
    "\n",
    "# o_file = f'{scenario_dir}/o.wav'\n",
    "# y_o, sr = lb.core.load(o_file)\n",
    "# # y_o = y_o[int(orch_start_sec*sr):int(orch_end_sec*sr)]\n",
    "# F_o = lb.feature.chroma_cens(y=y_o, sr=sr, hop_length=hop_length)\n",
    "# F_o = F_o[:,orch_start_frm:orch_end_frm]\n",
    "# F_o = F_o[:,::downsample]\n",
    "\n",
    "# po_file = f'{scenario_dir}/po.wav'\n",
    "# y_po, sr = lb.core.load(po_file)\n",
    "# # y_po = y_po[int(orch_start_sec*sr):]\n",
    "# F_po = lb.feature.chroma_cens(y=y_po, sr=sr, hop_length=hop_length)\n",
    "# F_po = F_po[:,orch_start_frm:orch_end_frm]\n",
    "# F_po = F_po[:,::downsample]\n",
    "\n",
    "# np.save(f'{cache_dir}/po_cqt.npy', F_po)\n",
    "# np.save(f'{cache_dir}/o_cqt.npy', F_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "985b3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_file = f'{scenario_dir}/p.wav'\n",
    "# y, sr = lb.core.load(p_file)\n",
    "\n",
    "# F_p = lb.feature.chroma_cens(y=y, sr=sr, hop_length=hop_length)[:,::downsample]\n",
    "# F_po = np.load(f'{cache_dir}/po_cqt.npy') # full mix features\n",
    "# F_o = np.load(f'{cache_dir}/o_cqt.npy') # orchestra\n",
    "\n",
    "# hop_sec = downsample * hop_length / sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ac7275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F_p.shape, F_o.shape, F_po.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e4951bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def calculate_cost_tensor(x, y, z):\n",
    "    '''\n",
    "    Calculates the cost tensor for the given piano, orchestra, and piano-orchestra features.\n",
    "    \n",
    "    Args:\n",
    "        x: submix features\n",
    "        y: submix features\n",
    "        z: full mix features\n",
    "    '''\n",
    "    x = x.T\n",
    "    y = y.T\n",
    "    z = z.T\n",
    "    cost_tensor = np.zeros((x.shape[0], y.shape[0], z.shape[0]))\n",
    "    for i in prange(x.shape[0]):\n",
    "        for j in prange(y.shape[0]):\n",
    "            for k in prange(z.shape[0]):\n",
    "                cost_tensor[i, j, k] = 1 - (np.dot(z[k,:], x[i,:] + y[j,:]) / (np.linalg.norm(z[k,:]) * np.linalg.norm(x[i,:] + y[j,:])))\n",
    "    return cost_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "392c575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def backtrace_flexdtw(D, B, steps, rstart, cstart, tstart):\n",
    "    '''\n",
    "    Backtraces through the cumulative cost matrix D starting from a specified location.\n",
    "    \n",
    "    Arguments:\n",
    "    D: cumulative cost matrix\n",
    "    B: backtrace matrix\n",
    "    steps: a numpy matrix specifying the allowable transitions.  It should be of\n",
    "            dimension (L, 2), where each row specifies (row step, col step)\n",
    "    rstart: the row index to start backtracking from\n",
    "    cstart: the column index to start backtracking from\n",
    "    \n",
    "    Outputs\n",
    "    path: a python list of (row, col) coordinates for the optimal path.\n",
    "    '''\n",
    "    pos = (rstart, cstart, tstart)\n",
    "    path = []\n",
    "    path.append(pos)\n",
    "    while(pos[0] != 0 and pos[1] != 0 and pos[2] != 0):\n",
    "        (row, col, tube) = pos\n",
    "        stepidx = B[row, col, tube]\n",
    "        (rstep, cstep, tstep) = steps[stepidx]\n",
    "        pos = (row-rstep, col-cstep, tube-tstep)\n",
    "        path.append(pos)\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "388ac764",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def find_best_endpoint(D, P, buffer):\n",
    "    '''\n",
    "    Determines the best location to begin backtracking from by comparing the average path cost\n",
    "    per manhattan block.\n",
    "    \n",
    "    Inputs\n",
    "    D: the cumulative cost matrix\n",
    "    P: the matrix specifying the starting location of the alignment path\n",
    "    buffer: specifies the length of a buffer region (in frames) to avoid short degenerate alignment paths\n",
    "        near the corners of the pairwise cost matrix.  This can be thought of as the minimum length that\n",
    "        needs to match in order to be considered a valid alignment path.\n",
    "    \n",
    "    Outputs\n",
    "    best_cost: the best average path cost per manhattan block\n",
    "    best_r: the row index of the best endpoint\n",
    "    best_c: the column index of the best endpoint\n",
    "    debug: debugging information for examining the average cost per manhattan block for each \n",
    "        of the candidate ending positions\n",
    "    '''\n",
    "    \n",
    "    # consider the top corner 3 faces as candidates\n",
    "    candidates = []\n",
    "    for i in range(buffer, D.shape[0]):\n",
    "        for j in range(buffer, D.shape[1]):\n",
    "            candidates.append((i, j, D.shape[2]-1))\n",
    "\n",
    "    for j in range(buffer, D.shape[1]):\n",
    "        for k in range(buffer, D.shape[2]):\n",
    "            candidates.append((D.shape[0]-1, j, k))\n",
    "\n",
    "    for i in range(buffer, D.shape[0]):\n",
    "        for k in range(buffer, D.shape[2]):\n",
    "            candidates.append((i, D.shape[1]-1, k))\n",
    "    \n",
    "    best_cost = np.inf\n",
    "    best_r, best_c, best_t = -1, -1, -1\n",
    "    # debug = []\n",
    "    debug = np.zeros_like(D)\n",
    "    \n",
    "    for i, (r,c,t) in enumerate(candidates):\n",
    "                \n",
    "        # get alignment start location\n",
    "        rstart, cstart, tstart = P[r,c,t]\n",
    "            \n",
    "        # calculate average cost per manhattan block\n",
    "        mdist = (r - rstart) + (c - cstart) + (t - tstart)# manhattan distance\n",
    "        avg_cost_per_mb = D[r,c,t] / mdist\n",
    "        \n",
    "        # keep best\n",
    "        if avg_cost_per_mb < best_cost:\n",
    "            best_cost = avg_cost_per_mb\n",
    "            best_r, best_c, best_t = r, c, t\n",
    "            \n",
    "        # debugging info\n",
    "        # TODO: updated this for 3D-FlexDTW\n",
    "        # if r == D.shape[0]-1:\n",
    "        #     debug.append((c-D.shape[1]+1, avg_cost_per_mb, r, c))\n",
    "        # else:\n",
    "        #     debug.append((D.shape[0]-1-r, avg_cost_per_mb, r, c))\n",
    "        debug[r,c,t] = avg_cost_per_mb\n",
    "    \n",
    "    return best_cost, best_r, best_c, best_t, debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee23b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def flexdtw(x, y, z, steps, weights, buffer = 1):\n",
    "    C = calculate_cost_tensor(x, y, z)\n",
    "    \n",
    "    # Run FlexDTW on the cost tensor\n",
    "    D = np.ones(C.shape) * np.inf\n",
    "    B = np.zeros(C.shape, dtype=np.int8)\n",
    "\n",
    "    # D[0, :, :] = np.inf\n",
    "    # D[:, 0, :] = np.inf\n",
    "    # D[:, :, 0] = np.inf\n",
    "    # for row in range(1,C.shape[0]):\n",
    "    #     for col in range(1, C.shape[1]):\n",
    "    #         for tube in range (1, C.shape[2]):\n",
    "    #             D[row, col, tube] = -np.inf\n",
    "    D[0,0,0] = C[0,0,0]\n",
    "\n",
    "    # DP\n",
    "    for row in range(1,C.shape[0]):\n",
    "        for col in range(1, C.shape[1]):\n",
    "            for tube in range (1, C.shape[2]):\n",
    "                mincost = np.inf\n",
    "                minidx = -1\n",
    "                bestrprev = -1\n",
    "                bestcprev = -1\n",
    "                besttprev = -1\n",
    "                for stepidx, step in enumerate(steps):\n",
    "                    (rstep, cstep, tstep) = step\n",
    "                    prevrow = row - rstep\n",
    "                    prevcol = col - cstep\n",
    "                    prevtube = tube - tstep\n",
    "                    if prevrow >= 0 and prevcol >= 0 and prevtube >= 0:\n",
    "                        \n",
    "                        # calculate avg cost per manhattan block\n",
    "                        pathcost = D[prevrow, prevcol, prevtube] + C[row, col, tube] * weights[stepidx]\n",
    "                        \n",
    "                        # select best transition based on avg cost per manhattan block\n",
    "                        if pathcost < mincost:\n",
    "                            mincost = pathcost\n",
    "                            minidx = stepidx\n",
    "                            bestrprev = prevrow\n",
    "                            bestcprev = prevcol\n",
    "                            besttprev = prevtube\n",
    "                            \n",
    "                D[row, col, tube] = D[bestrprev, bestcprev, besttprev] + C[row, col, tube] * weights[minidx]\n",
    "                B[row, col, tube] = minidx\n",
    "\n",
    "    best_cost = D[-1, -1, -1]\n",
    "    path = backtrace_flexdtw(D, B, steps, x.shape[1]-1, y.shape[1]-1, z.shape[1]-1)\n",
    "    path.reverse()\n",
    "    path = np.array(path)\n",
    "    return best_cost, path.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da7538a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_cost, path = flexdtw(F_p, F_o, F_po, np.array([\n",
    "#         [1,1,1],\n",
    "#         [2,1,1],\n",
    "#         [1,2,1],\n",
    "#         [1,1,2],\n",
    "#         [2,2,1],\n",
    "#         [2,1,2],\n",
    "#         [1,2,2],\n",
    "#     ]),\n",
    "#     np.array([3,4,4,4,5,5,5]), buffer = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eb3e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt_line = k3d.line(path.T, width=0.1, color=0xff99cc)\n",
    "\n",
    "# start_label = k3d.label('0,0,0', position=(0, 0, 0))\n",
    "# # end_label = k3d.label('{}, {}, {}'.format(x.shape[1], y.shape[1], z.shape[1]), position=(x.shape[1], y.shape[1], z.shape[1]))\n",
    "# end_label = k3d.label('{}, {}, {}'.format(F_p.shape[1], F_o.shape[1], F_po.shape[1]), position=(F_p.shape[1], F_o.shape[1], F_po.shape[1]))\n",
    "# # end_label = k3d.label('{}, {}, {}'.format(context_size, context_size, context_size), position=(context_size, context_size, context_size))\n",
    "\n",
    "# plot = k3d.plot()\n",
    "# plot += plt_line\n",
    "# plot += start_label\n",
    "# plot += end_label\n",
    "# plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cc8dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost_tensor = calculate_cost_tensor(F_p, F_o, F_po)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de8844f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(cost_tensor[:,0,:], origin='lower')\n",
    "# plt.colorbar(orientation='horizontal')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1798818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99ce37f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt_volume = k3d.volume(cost_tensor, bounds=[0, cost_tensor.shape[0], 0, cost_tensor.shape[1], 0, cost_tensor.shape[2]])\n",
    "\n",
    "# plot = k3d.plot()\n",
    "# plot += plt_volume\n",
    "# plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eee080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vol = debug\n",
    "# plt_volume = k3d.volume(vol, bounds=[0, vol.shape[0], 0, vol.shape[1], 0, vol.shape[2]])\n",
    "# # plt_volume = k3d.volume(calculate_cost_tensor(F_p, F_o, F_po))\n",
    "\n",
    "# plot = k3d.plot()\n",
    "# plot += plt_volume\n",
    "# plot.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3fe4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # infer piano-orchestra alignment\n",
    "# wp_AC = np.vstack((path[0],path[1]))\n",
    "# wp_AC_sec = wp_AC*hop_sec\n",
    "# wp_AC_sec[1,:] += scenario_info['oStart']\n",
    "# np.save(f'{out_dir}/hyp.npy', wp_AC_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82cbce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.vstack((path[0],path[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6473c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_utils.get_orchestra_query_boundaries(scenario_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a5af517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import matplotlib.pyplot as plt\n",
    "\n",
    "# # alignment1 = wp_AB_\n",
    "# # alignment2 = wp_BC\n",
    "# # alignment3 = wp_AC\n",
    "\n",
    "# # plt.plot(path[0], path[2], label='AB')\n",
    "# # plt.plot(path[1], path[2], label='BC')\n",
    "# plt.plot(path[1], path[0], label='AC')\n",
    "# plt.xlabel('Source Time')\n",
    "# plt.ylabel('Aligned Time')\n",
    "# plt.ylim(ymin=0)\n",
    "# plt.xlim(xmin=0)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2554f310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
