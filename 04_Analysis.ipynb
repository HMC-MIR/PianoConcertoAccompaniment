{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48653ed7",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64615ac2",
   "metadata": {},
   "source": [
    "This notebook provides several analyses to gain deeper intuition into system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354dbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3809e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lb\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4ab1e",
   "metadata": {},
   "source": [
    "## Visualizing errors over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288f865",
   "metadata": {},
   "source": [
    "The first analysis is to simply visualize the magnitude of alignment errors over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba41769",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DIR = 'eval/simpleOfflineDTW' # eval directory to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{EVAL_DIR}/errs.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario_id in d:\n",
    "    errs = d[scenario_id][0] # downbeat alignment errors \n",
    "    measures = len(errs)\n",
    "    plt.plot(np.arange(measures), errs)\n",
    "plt.xlabel('Measure Number')\n",
    "plt.ylabel('Alignment Error (sec)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a423a7",
   "metadata": {},
   "source": [
    "There are large errors at the beginning.  This should not be happening, since we assume that the pianist comes in at the right time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8544043e",
   "metadata": {},
   "source": [
    "## Generating sonifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff2755",
   "metadata": {},
   "source": [
    "The second analysis is to use the estimated alignment to time-scaled modify the orchestral recording.  We generate an audio file that contains the original piano recording on the left channel and the time-scale modified accompaniment on the right channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import tsm_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a50007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_separate_channels(left_channel, right_channel):\n",
    "    '''\n",
    "    Merges two mono audio waveforms into a stereo audio waveform.  If the two waveforms differ\n",
    "    in length, the longer of the two is truncated.\n",
    "    \n",
    "    Inputs\n",
    "    left_channel: the audio waveform for the left channel\n",
    "    right_channel: the audio waveform for the right channel\n",
    "    '''\n",
    "    N = min(len(left_channel), len(right_channel))\n",
    "    mixed = np.zeros((N, 2))\n",
    "    mixed[:,0] = left_channel[0:N]\n",
    "    mixed[:,1] = right_channel[0:N]\n",
    "    return mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a737c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sonifyWithAccompaniment(piano_audiofile, orch_audiofile, align_file, downsample, hop_len, outfile = None):\n",
    "    '''\n",
    "    Generates a stereo audio recording with the original piano recording on one channel\n",
    "    and the time-scale modified orchestral recording on the other channel.\n",
    "    \n",
    "    Inputs\n",
    "    piano_audiofile: filepath to the audio recording containing the piano (only) part\n",
    "    orch_audiofile: filepath to the audio recording containing the orchestra (only) part\n",
    "    align_file: filepath to the .npy file specifying the warping path between piano and orchestra parts\n",
    "    downsample: downsample the warping path by this factor to smooth out the TSM\n",
    "    hop_len: specifies the hop length in seconds between frames, needed to convert warping path to timestamps\n",
    "    outfile: the output audio file to generate\n",
    "    '''\n",
    "    y_piano, sr = lb.load(piano_audiofile)\n",
    "    y_orch, sr = lb.load(orch_audiofile)\n",
    "    wp = np.load(align_file) # 2xN array specifying piano-orchestra alignment (in frames)\n",
    "    wp = np.flipud(wp[:,0::downsample]) * hop_len # convert to sec, downsample for smoothing\n",
    "    y_orch_tsm = tsm_tools.tsmvar_hybrid(y_orch, wp)\n",
    "    y_mixed = mix_separate_channels(y_piano, y_orch_tsm)\n",
    "    if outfile:\n",
    "        sf.write(outfile, y_mixed, sr, subtype='PCM_24')\n",
    "    return y_mixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c848a",
   "metadata": {},
   "source": [
    "Here is example usage for sonifying a single scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# piano_file = f'scenarios/s1/p.wav'\n",
    "# orch_file = 'scenarios/s1/o.mp3'\n",
    "# align_file = 'experiments/simpleOfflineDTW/s1/hyp.npy'\n",
    "# downsample = 20\n",
    "# sr = 22050\n",
    "# hop_len = 512./sr\n",
    "# y = sonifyWithAccompaniment(piano_file, orch_file, align_file, downsample, hop_len)\n",
    "# ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sonifyWithAccompaniment_batch(scenarios_dir, exp_dir, downsample, hop_len, outdir):\n",
    "    '''\n",
    "    Generates stereo recordings of piano (left channel) and time-scale modified orchestra recordings\n",
    "    (right channel) for all scenarios.\n",
    "    \n",
    "    Inputs\n",
    "    scenarios_dir: directory containing all scenario directories\n",
    "    exp_dir: directory containing all the hypothesis alignments\n",
    "    downsample: downsample the warping path by this factor to smooth out the TSM\n",
    "    hop_len: specifies the hop length in seconds between frames, needed to convert warping path to timestamps\n",
    "    outdir: directory to put the generated audio files\n",
    "    '''\n",
    "    assert not os.path.exists(outdir)\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "    scenario_ids = list(pd.read_csv(f'{scenarios_dir}/scenarios.summary', header=None, sep=' ')[0])\n",
    "    for scenario_id in scenario_ids:\n",
    "        piano_file = f'{scenarios_dir}/{scenario_id}/p.wav'\n",
    "        orch_file = f'{scenarios_dir}/{scenario_id}/o.mp3'\n",
    "        align_file = f'{exp_dir}/{scenario_id}/hyp.npy'\n",
    "        out_file = f'{outdir}/{scenario_id}.wav'\n",
    "        sonifyWithAccompaniment(piano_file, orch_file, align_file, downsample, hop_len, out_file)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b6d01",
   "metadata": {},
   "source": [
    "The following two cells generate sonifications for all scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a874ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIOS_DIR = 'scenarios'\n",
    "EXP_DIR = 'experiments/simpleOfflineDTW'\n",
    "SONIFY_DIR = f'{EXP_DIR}/sonify'\n",
    "downsample = 20\n",
    "hop_len = 512./22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonifyWithAccompaniment_batch(SCENARIOS_DIR, EXP_DIR, downsample, hop_len, SONIFY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564533e",
   "metadata": {},
   "source": [
    "Listen to a selected example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(f'{SONIFY_DIR}/s1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212141c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accompaniment",
   "language": "python",
   "name": "accompaniment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
