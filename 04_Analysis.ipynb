{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48653ed7",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64615ac2",
   "metadata": {},
   "source": [
    "This notebook provides several analyses to gain deeper intuition into system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354dbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3809e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lb\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4ab1e",
   "metadata": {},
   "source": [
    "## Visualizing errors over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288f865",
   "metadata": {},
   "source": [
    "The first analysis is to simply visualize the magnitude of alignment errors over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba41769",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DIR = 'eval/simpleOfflineDTW' # eval directory to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{EVAL_DIR}/errs.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scenario_id in d:\n",
    "    errs = d[scenario_id][0] # downbeat alignment errors \n",
    "    measures = len(errs)\n",
    "    plt.plot(np.arange(measures), errs)\n",
    "plt.xlabel('Measure Number')\n",
    "plt.ylabel('Alignment Error (sec)')\n",
    "plt.grid(linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8544043e",
   "metadata": {},
   "source": [
    "## Generating sonifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff2755",
   "metadata": {},
   "source": [
    "The second analysis is to use the estimated alignment to time-scaled modify the orchestral recording.  We generate an audio file that contains the original piano recording on the left channel and the time-scale modified accompaniment on the right channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import tsm_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_wp(align_file, hop_len, downsample, dur1, dur2):\n",
    "    '''\n",
    "    Performs preprocessing on a given warping path to ensure that the time-scale modification\n",
    "    can be carried out.\n",
    "    \n",
    "    Inputs\n",
    "    align_file: filepath to the .npy file specifying the warping path (in frames) between the two audio files\n",
    "    hop_len: specifies the hop length in seconds between frames, needed to convert warping path to timestamps\n",
    "    downsample: downsample the warping path by this factor to smooth out the TSM\n",
    "    dur1: the duration of file1 (seconds)\n",
    "    dur2: the duration of file2 (seconds)\n",
    "    \n",
    "    Returns the modified warping path.\n",
    "    '''\n",
    "    wp = np.load(align_file) # 2xN array specifying file1-file2 alignment (in frames)\n",
    "    wp = wp[:,0::downsample] * hop_len # downsample for smoothing, convert to sec    \n",
    "    if not np.array_equal(wp[:,0], [0,0]):\n",
    "        wp = np.hstack((np.array([0,0]).reshape((2,-1)), wp)) # alignment must start at (0,0)\n",
    "    wp = np.hstack((wp, np.array([dur1, dur2]).reshape((2,-1)))) # alignment must extend to ends    \n",
    "    return wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a50007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_separate_channels(left_channel, right_channel):\n",
    "    '''\n",
    "    Merges two mono audio waveforms into a stereo audio waveform.  If the two waveforms differ\n",
    "    in length, the longer of the two is truncated.\n",
    "    \n",
    "    Inputs\n",
    "    left_channel: the audio waveform for the left channel\n",
    "    right_channel: the audio waveform for the right channel\n",
    "    '''\n",
    "    N = min(len(left_channel), len(right_channel))\n",
    "    mixed = np.zeros((N, 2))\n",
    "    mixed[:,0] = left_channel[0:N]\n",
    "    mixed[:,1] = right_channel[0:N]\n",
    "    return mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a737c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sonifyWithAccompaniment(audiofile1, audiofile2, align_file, downsample, hop_len, outfile = None):\n",
    "    '''\n",
    "    Generates a stereo audio recording with one audio recording on the left channel and another audio\n",
    "    recording on the other channel, where time-scale modification has been applied to the latter so\n",
    "    that the two recordings are appropriately synchronized.\n",
    "    \n",
    "    Inputs\n",
    "    audiofile1: filepath to the audio recording that will remain unmodified\n",
    "    audiofile2: filepath to the audio recording that will be time-scaled modified\n",
    "    align_file: filepath to the .npy file specifying the warping path (in frames) between the two audio files\n",
    "    downsample: downsample the warping path by this factor to smooth out the TSM\n",
    "    hop_len: specifies the hop length in seconds between frames, needed to convert warping path to timestamps\n",
    "    outfile: the output audio file to generate\n",
    "    '''\n",
    "    y1, sr = lb.load(audiofile1)\n",
    "    y2, sr = lb.load(audiofile2)\n",
    "    wp = get_preprocessed_wp(align_file, hop_len, downsample, len(y1)/sr, len(y2)/sr) # file1-file2 alignment in sec\n",
    "    y2_tsm = tsm_tools.tsmvar_hybrid(y2, np.flipud(wp))\n",
    "    y_mixed = mix_separate_channels(y1, y2_tsm)\n",
    "    if outfile:\n",
    "        sf.write(outfile, y_mixed, sr, subtype='PCM_16')\n",
    "    return y_mixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c848a",
   "metadata": {},
   "source": [
    "Here is example usage for sonifying a single scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audiofile1 = f'scenarios/s1/p.wav'\n",
    "audiofile2 = 'scenarios/s1/o.wav'\n",
    "align_file = 'experiments/simpleOfflineDTW2/s2/hyp.npy' # p-o alignment\n",
    "outfile = 'test.wav'\n",
    "downsample = 20\n",
    "sr = 22050\n",
    "hop_len = 512./sr\n",
    "y = sonifyWithAccompaniment(audiofile1, audiofile2, align_file, downsample, hop_len, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebe073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified example\n",
    "audiofile1 = f'scenarios/s1/p.wav'\n",
    "audiofile2 = 'scenarios/s1/po.wav'\n",
    "align_file = 'experiments/offlineDTW/cache/rach2_mov1_O1_PO1/po_o_align.npy' # po-o alignment\n",
    "outfile = 'test.wav'\n",
    "downsample = 20\n",
    "sr = 22050\n",
    "hop_len = 512./sr\n",
    "y = sonifyWithAccompaniment(audiofile1, audiofile2, align_file, downsample, hop_len, outfile)\n",
    "y[:,0] = y[:,0]/10\n",
    "sf.write(outfile, y, sr, subtype='PCM_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sonifyWithAccompaniment_batch(scenarios_dir, exp_dir, downsample, hop_len, outdir):\n",
    "    '''\n",
    "    Generates stereo recordings of piano (left channel) and time-scale modified orchestra recordings\n",
    "    (right channel) for all scenarios.\n",
    "    \n",
    "    Inputs\n",
    "    scenarios_dir: directory containing all scenario directories\n",
    "    exp_dir: directory containing all the hypothesis alignments\n",
    "    downsample: downsample the warping path by this factor to smooth out the TSM\n",
    "    hop_len: specifies the hop length in seconds between frames, needed to convert warping path to timestamps\n",
    "    outdir: directory to put the generated audio files\n",
    "    '''\n",
    "    assert not os.path.exists(outdir)\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "    scenario_ids = list(pd.read_csv(f'{scenarios_dir}/scenarios.summary', header=None, sep=' ')[0])\n",
    "    for scenario_id in scenario_ids:\n",
    "        piano_file = f'{scenarios_dir}/{scenario_id}/p.wav'\n",
    "        orch_file = f'{scenarios_dir}/{scenario_id}/o.mp3'\n",
    "        align_file = f'{exp_dir}/{scenario_id}/hyp.npy'\n",
    "        out_file = f'{outdir}/{scenario_id}.wav'\n",
    "        sonifyWithAccompaniment(piano_file, orch_file, align_file, downsample, hop_len, out_file)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b6d01",
   "metadata": {},
   "source": [
    "The following two cells generate sonifications for all scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a874ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIOS_DIR = 'scenarios'\n",
    "EXP_DIR = 'experiments/simpleOfflineDTW2'\n",
    "SONIFY_DIR = f'{EXP_DIR}/sonify'\n",
    "downsample = 20\n",
    "hop_len = 512./22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonifyWithAccompaniment_batch(SCENARIOS_DIR, EXP_DIR, downsample, hop_len, SONIFY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564533e",
   "metadata": {},
   "source": [
    "Listen to a selected example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(f'{SONIFY_DIR}/s1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212141c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accompaniment",
   "language": "python",
   "name": "accompaniment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
