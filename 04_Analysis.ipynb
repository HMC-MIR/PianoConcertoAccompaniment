{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48653ed7",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64615ac2",
   "metadata": {},
   "source": [
    "This notebook provides several analyses to gain deeper intuition into system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354dbf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3809e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lb\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "import os.path\n",
    "import import_ipynb\n",
    "import system_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4ab1e",
   "metadata": {},
   "source": [
    "## Visualizing errors over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288f865",
   "metadata": {},
   "source": [
    "The first analysis is to simply visualize the magnitude of alignment errors over time for a specific concerto movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47782770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_errors_over_time(eval_dir, scenarios_summary, fullmix_id):\n",
    "    '''\n",
    "    Visualize the magnitude of alignment error over time for a given full mix recording.  \n",
    "    Measures that are not evaluated will be displayed as having zero error.\n",
    "    \n",
    "    Inputs\n",
    "    eval_dir: the eval directory containing alignment error information\n",
    "    scenarios_summary: filepath specifying the scenarios.summary file\n",
    "    fullmix_id: id specifying the full mix recording of interest, e.g. 'rach2_mov1_PO2'\n",
    "    '''\n",
    "\n",
    "    # parse relevant files\n",
    "    with open(f'{eval_dir}/errs.pkl', 'rb') as f:\n",
    "        errors_info = pickle.load(f) # key: scenario_id, value: (errs, measNums)\n",
    "    scenarios_info = system_utils.get_scenario_info(scenarios_summary) # key: scenario_id, value: dict with scenario info\n",
    "    \n",
    "    # aggregate data to display\n",
    "    data = {} # key: tsm_factor, value: dict whose key is the measure number and whose value is the align error\n",
    "    for scenario_id in scenarios_info:\n",
    "        if fullmix_id in scenarios_info[scenario_id]['po']:\n",
    "            tsm_factor = scenarios_info[scenario_id]['p'].split('/')[-2] # e.g. 'tsm0.80'\n",
    "            errs, measNums = errors_info[scenario_id] # only contains evaluation measures\n",
    "            if tsm_factor not in data:\n",
    "                data[tsm_factor] = {}\n",
    "            for err, measNum in zip(errs, measNums):\n",
    "                data[tsm_factor][measNum] = err\n",
    "    \n",
    "    # display non-evaluated measures as having 0 error\n",
    "    for tsm_factor in data:\n",
    "        maxMeasNum = np.max(list(data[tsm_factor].keys()))\n",
    "        for i in range(maxMeasNum):\n",
    "            if i not in data[tsm_factor]:\n",
    "                data[tsm_factor][i] = 0\n",
    "   \n",
    "    # show alignment error vs measure number\n",
    "    fig, axs = plt.subplots(len(data.keys()), sharex=True, sharey=True)\n",
    "    for i, tsm_factor in enumerate(data):\n",
    "        measures = sorted(data[tsm_factor].keys())\n",
    "        errors = [data[tsm_factor][m] for m in measures]\n",
    "        axs[i].plot(measures, errors)\n",
    "        axs[i].set_title(f'{fullmix_id} - {tsm_factor}')\n",
    "        axs[i].grid(linestyle='--')\n",
    "    plt.xlabel('Measure Number')\n",
    "    plt.ylabel('Alignment Error (sec)')\n",
    "    plt.ylim([-2.5, 2.5])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba41769",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DIR = 'eval/offlineDTW' # eval directory to visualize\n",
    "SCENARIOS_SUMMARY = 'scenarios/scenarios.summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd401af",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = visualize_errors_over_time(EVAL_DIR, SCENARIOS_SUMMARY, fullmix_id = 'rach2_mov1_PO1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8544043e",
   "metadata": {},
   "source": [
    "## Generating sonifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff2755",
   "metadata": {},
   "source": [
    "The second analysis is to use the estimated alignment to time-scaled modify the orchestral recording.  We generate an audio file that contains the original piano recording on the left channel and the time-scale modified accompaniment on the right channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import tsm_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_wp(align_file, hop_len, downsample, dur1, dur2):\n",
    "    '''\n",
    "    Performs preprocessing on a given warping path to ensure that the time-scale modification\n",
    "    can be carried out.\n",
    "    \n",
    "    Inputs\n",
    "    align_file: filepath to the .npy file specifying the warping path (in frames) between the two audio files\n",
    "    hop_len: specifies the hop length in seconds between frames, needed to convert warping path to timestamps\n",
    "    downsample: downsample the warping path by this factor to smooth out the TSM\n",
    "    dur1: the duration of file1 (seconds)\n",
    "    dur2: the duration of file2 (seconds)\n",
    "    \n",
    "    Returns the modified warping path.\n",
    "    '''\n",
    "    wp = np.load(align_file) # 2xN array specifying file1-file2 alignment (in frames)\n",
    "    wp = wp[:,0::downsample] * hop_len # downsample for smoothing, convert to sec    \n",
    "    if not np.array_equal(wp[:,0], [0,0]):\n",
    "        wp = np.hstack((np.array([0,0]).reshape((2,-1)), wp)) # alignment must start at (0,0)\n",
    "    wp = np.hstack((wp, np.array([dur1, dur2]).reshape((2,-1)))) # alignment must extend to ends    \n",
    "    return wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a50007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_separate_channels(left_channel, right_channel):\n",
    "    '''\n",
    "    Merges two mono audio waveforms into a stereo audio waveform.  If the two waveforms differ\n",
    "    in length, the longer of the two is truncated.\n",
    "    \n",
    "    Inputs\n",
    "    left_channel: the audio waveform for the left channel\n",
    "    right_channel: the audio waveform for the right channel\n",
    "    '''\n",
    "    N = min(len(left_channel), len(right_channel))\n",
    "    mixed = np.zeros((N, 2))\n",
    "    mixed[:,0] = left_channel[0:N]\n",
    "    mixed[:,1] = right_channel[0:N]\n",
    "    return mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a737c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sonifyWithAccompaniment(audiofile1, audiofile2, align_file, downsample, hop_len, outfile = None):\n",
    "    '''\n",
    "    Generates a stereo audio recording with one audio recording on the left channel and another audio\n",
    "    recording on the other channel, where time-scale modification has been applied to the latter so\n",
    "    that the two recordings are appropriately synchronized.\n",
    "    \n",
    "    Inputs\n",
    "    audiofile1: filepath to the audio recording that will remain unmodified\n",
    "    audiofile2: filepath to the audio recording that will be time-scaled modified\n",
    "    align_file: filepath to the .npy file specifying the warping path (in frames) between the two audio files\n",
    "    downsample: downsample the warping path by this factor to smooth out the TSM\n",
    "    hop_len: specifies the hop length in seconds between frames, needed to convert warping path to timestamps\n",
    "    outfile: the output audio file to generate\n",
    "    '''\n",
    "    y1, sr = lb.load(audiofile1)\n",
    "    y2, sr = lb.load(audiofile2)\n",
    "    wp = get_preprocessed_wp(align_file, hop_len, downsample, len(y1)/sr, len(y2)/sr) # file1-file2 alignment in sec\n",
    "    y2_tsm = tsm_tools.tsmvar_hybrid(y2, np.flipud(wp))\n",
    "    y_mixed = mix_separate_channels(y1, y2_tsm)\n",
    "    if outfile:\n",
    "        sf.write(outfile, y_mixed, sr, subtype='PCM_16')\n",
    "    return y_mixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c848a",
   "metadata": {},
   "source": [
    "Here is example usage for sonifying a single scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52e4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audiofile1 = f'scenarios/s1/p.wav'\n",
    "audiofile2 = 'scenarios/s1/o.wav'\n",
    "align_file = 'experiments/simpleOfflineDTW2/s2/hyp.npy' # p-o alignment\n",
    "outfile = 'test.wav'\n",
    "downsample = 20\n",
    "sr = 22050\n",
    "hop_len = 512./sr\n",
    "y = sonifyWithAccompaniment(audiofile1, audiofile2, align_file, downsample, hop_len, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c622a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified example\n",
    "audiofile1 = f'scenarios/s1/p.wav'\n",
    "audiofile2 = 'scenarios/s1/po.wav'\n",
    "#align_file = 'experiments/offlineDTW/cache/rach2_mov1_O1_PO1/po_o_align.npy' # po-o alignment\n",
    "align_file = 'experiments/offlineDTW/s1/p_po_align.npy' # p-po alignment\n",
    "outfile = 'test.wav'\n",
    "downsample = 20\n",
    "sr = 22050\n",
    "hop_len = 512./sr\n",
    "y = sonifyWithAccompaniment(audiofile1, audiofile2, align_file, downsample, hop_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:,0] = y[:,0]/3\n",
    "sf.write(outfile, y, sr, subtype='PCM_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sonifyWithAccompaniment_batch(scenarios_dir, exp_dir, downsample, hop_len, outdir):\n",
    "    '''\n",
    "    Generates stereo recordings of piano (left channel) and time-scale modified orchestra recordings\n",
    "    (right channel) for all scenarios.\n",
    "    \n",
    "    Inputs\n",
    "    scenarios_dir: directory containing all scenario directories\n",
    "    exp_dir: directory containing all the hypothesis alignments\n",
    "    downsample: downsample the warping path by this factor to smooth out the TSM\n",
    "    hop_len: specifies the hop length in seconds between frames, needed to convert warping path to timestamps\n",
    "    outdir: directory to put the generated audio files\n",
    "    '''\n",
    "    assert not os.path.exists(outdir)\n",
    "    os.makedirs(outdir)\n",
    "    \n",
    "    scenario_ids = list(pd.read_csv(f'{scenarios_dir}/scenarios.summary', header=None, sep=' ')[0])\n",
    "    for scenario_id in scenario_ids:\n",
    "        piano_file = f'{scenarios_dir}/{scenario_id}/p.wav'\n",
    "        orch_file = f'{scenarios_dir}/{scenario_id}/o.mp3'\n",
    "        align_file = f'{exp_dir}/{scenario_id}/hyp.npy'\n",
    "        out_file = f'{outdir}/{scenario_id}.wav'\n",
    "        sonifyWithAccompaniment(piano_file, orch_file, align_file, downsample, hop_len, out_file)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b6d01",
   "metadata": {},
   "source": [
    "The following two cells generate sonifications for all scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a874ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENARIOS_DIR = 'scenarios'\n",
    "EXP_DIR = 'experiments/simpleOfflineDTW2'\n",
    "SONIFY_DIR = f'{EXP_DIR}/sonify'\n",
    "downsample = 20\n",
    "hop_len = 512./22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonifyWithAccompaniment_batch(SCENARIOS_DIR, EXP_DIR, downsample, hop_len, SONIFY_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564533e",
   "metadata": {},
   "source": [
    "Listen to a selected example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(f'{SONIFY_DIR}/s1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a212141c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accompaniment",
   "language": "python",
   "name": "accompaniment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
