{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6a79b99",
   "metadata": {},
   "source": [
    "# Frame Classification + Dense-Sparse DTW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cb97f15",
   "metadata": {},
   "source": [
    "This notebook implements an approach that first classifies individual frames as piano or orchestra based on a GMM, and then estimates the P-PO and O-PO alignments using dense-sparse DTW."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ff6b5de",
   "metadata": {},
   "source": [
    "Here is a summary of the source separation + DTW approach:\n",
    "- Offline processing:\n",
    "    - Source separation is used to split the full mix recording into estimated piano and estimated orchestra recordings.\n",
    "    - The orchestra and estimated orchestra are aligned with standard DTW using chroma features.\n",
    "- Online processing: The solo piano and estimated piano are aligned with DTW, and the predicted alignment is then used to infer the corresponding alignment between the piano and orchestra recordings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70d1df11",
   "metadata": {},
   "source": [
    "## Offline Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "610e1142",
   "metadata": {},
   "source": [
    "The offline processing is the same as in the simple offline DTW system.  In the offline processing stage, three things are computed and stored in the `cache/` folder:\n",
    "- chroma features for the orchestra recording\n",
    "- chroma features for the estimated orchestra recording (i.e. the result of performing source separation on the full mix recording)\n",
    "- predicted DTW alignment between the orchestra and estimated orchestra recordings\n",
    "\n",
    "NOTE: because we do not have the code to do the source separation, the precomputed source separated files should be placed in a folder somewhere in the root `audio/` folder.  This must be done BEFORE the following code can be run. This is an example of what files should be present in the directory *before* running the offline processing step (shown for the HDemucs source separation model):\n",
    "\n",
    "\n",
    "```\n",
    "separation\n",
    "├── HDemucs\n",
    "    ├── bach5_mov1_PO1_O.wav\n",
    "    ├── bach5_mov1_PO1_P.wav\n",
    "    ├── bach5_mov1_PO2_O.wav\n",
    "    ├── bach5_mov1_PO2_P.wav\n",
    "    ├── beeth1_mov1_PO1_O.wav\n",
    "    ├── beeth1_mov1_PO1_P.wav\n",
    "    ├── beeth1_mov1_PO2_O.wav\n",
    "    ├── beeth1_mov1_PO2_P.wav\n",
    "    ├── mozart21_mov1_PO1_O.wav\n",
    "    ├── mozart21_mov1_PO1_P.wav\n",
    "    ├── mozart21_mov1_PO2_O.wav\n",
    "    ├── mozart21_mov1_PO2_P.wav\n",
    "    ├── rach2_mov1_PO1_O.wav\n",
    "    ├── rach2_mov1_PO1_P.wav\n",
    "    ├── rach2_mov1_PO2_O.wav\n",
    "    └── rach2_mov1_PO2_P.wav\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0653f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import import_ipynb\n",
    "import librosa as lb\n",
    "from sklearn import mixture\n",
    "import system_utils\n",
    "import align_tools\n",
    "import os\n",
    "import os.path\n",
    "import subprocess\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from hmc_mir.align import dtw\n",
    "import time\n",
    "from numba import jit, njit, prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e6185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_processing(scenario_dir, cache_dir, hop_length, steps, weights):\n",
    "    '''Carries out the same offline processing steps as the simple offline DTW system.\n",
    "    \n",
    "    Args\n",
    "        scenario_dir: The scenario directory to process\n",
    "        cache_dir: The location of the cache directory\n",
    "        hop_length: The hop length in samples used when computing chroma features\n",
    "        steps: an L x 2 array specifying the allowable DTW transitions\n",
    "        weights: a length L array specifying the DTW transition weights\n",
    "        separation_dir: directory where the pre-separated audio files are stored\n",
    "    \n",
    "    This function will store the computed chroma features and estimated alignment in the cache folder.\n",
    "    '''\n",
    "\n",
    "    # setup\n",
    "    system_utils.verify_scenario_dir(scenario_dir)\n",
    "\n",
    "    if os.path.exists(cache_dir):\n",
    "        # print(f'{cache_dir} has already been processed.  Skipping.')\n",
    "        pass\n",
    "    else:\n",
    "        # setup\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "        # compute orchestra features\n",
    "        o_file = f'{scenario_dir}/o.wav'\n",
    "        y_o, sr = lb.core.load(o_file)\n",
    "        F_o_chroma = lb.feature.chroma_cqt(y=y_o, sr=sr, hop_length=hop_length, norm=2) \n",
    "        F_o_mfcc = lb.feature.mfcc(y=y_o, sr=sr, hop_length=hop_length)\n",
    "\n",
    "        # compute full mix features\n",
    "        po_file = f'{scenario_dir}/po.wav'\n",
    "        y_po, sr = lb.core.load(po_file)\n",
    "        F_po_chroma = lb.feature.chroma_cqt(y=y_po, sr=sr, hop_length=hop_length, norm=2)\n",
    "        F_po_mfcc = lb.feature.mfcc(y=y_po, sr=sr, hop_length=hop_length)\n",
    "      \n",
    "        # fit GMM model to orchestra MFCCs\n",
    "        gmm_O = mixture.GaussianMixture(n_components=10, covariance_type='diag', random_state=0)\n",
    "        gmm_O.fit(F_o_mfcc[1:13,:].T)\n",
    "        \n",
    "        # save to cache - TO DO\n",
    "        np.save(f'{cache_dir}/o_chroma.npy', F_o_chroma)\n",
    "        np.save(f'{cache_dir}/o_mfcc.npy', F_o_mfcc)\n",
    "        np.save(f'{cache_dir}/po_chroma.npy', F_po_chroma)\n",
    "        np.save(f'{cache_dir}/po_mfcc.npy', F_po_mfcc)\n",
    "        with open(f'{cache_dir}/gmm_O.pkl','wb') as f:\n",
    "            pickle.dump(gmm_O,f)\n",
    "        #np.save(f'{cache_dir}/runtime_o_po.npy', t_end - t_start)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d231c4-fd2b-476a-a18f-ee2fa86eb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def dtw_sparse_subseq(C, gaplens):\n",
    "    '''\n",
    "    A variant of subsequence DTW that aligns a selected subset of query features against a longer reference sequence.\n",
    "    The query sequence can start and end anywhere in the reference sequence, and the alignment handles gaps between\n",
    "    selected query features.\n",
    "    \n",
    "    Inputs:\n",
    "        C: an MxN matrix of pairwise costs, where M is the length of the (selected) query features and N is the length of\n",
    "           the reference sequence\n",
    "        gaplens: an array of length M specifying the gap lengths between selected features\n",
    "\n",
    "    Returns:\n",
    "        D: cumulative cost matrix, size MxN\n",
    "        B: backtrace matrix of size MxN, each element specifies either the step index (if dense matching)\n",
    "           or the number of reference frames skipped (if sparse matching)\n",
    "        path: a numpy array of (row, col) coordinates for the optimal path\n",
    "    '''\n",
    "    D = np.ones(C.shape) * np.inf\n",
    "    B = np.zeros(C.shape, dtype=np.int32)\n",
    "    steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "    weights = np.array([1,1,2])\n",
    "\n",
    "    D[0, :] = C[0,:]\n",
    "\n",
    "    for row in range(1, C.shape[0]):\n",
    "        for col in range(1, C.shape[1]):\n",
    "            \n",
    "            if row >= 2 and gaplens[row-2] == 1 and gaplens[row-1] == 1:\n",
    "                \n",
    "                # dense matching\n",
    "                bestCost = D[row, col]\n",
    "                bestCostIndex = -1\n",
    "                for stepIndex in range(steps.shape[0]):\n",
    "                    if row - steps[stepIndex][0] >= 0 and col - steps[stepIndex][1] >= 0:\n",
    "                        costForStep = C[row, col] * weights[stepIndex] + D[row - steps[stepIndex][0], col - steps[stepIndex][1]]\n",
    "                        if costForStep < bestCost:\n",
    "                            bestCost = costForStep\n",
    "                            bestCostIndex = stepIndex\n",
    "                D[row, col] = bestCost\n",
    "                B[row, col] = bestCostIndex\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # sparse matching\n",
    "                # cstep_lbound = int(np.ceil(gaplens[row-1]/2))\n",
    "                # cstep_ubound = gaplens[row-1]*2 + 1\n",
    "                # bestCost = D[row, col]\n",
    "                # for cstep in range(cstep_lbound, cstep_ubound):\n",
    "                #     rprev = row - 1\n",
    "                #     cprev = col - cstep\n",
    "                #     if cprev >= 0:\n",
    "                #         costForStep = C[row, col] + D[rprev, cprev]\n",
    "                #         if costForStep < bestCost:\n",
    "                #             bestCost = costForStep\n",
    "                #             bestCostIndex = cstep\n",
    "                # D[row, col] = bestCost\n",
    "                # B[row, col] = bestCostIndex\n",
    "\n",
    "                crange_lbound = max(col - gaplens[row-1]*2, 0)\n",
    "                crange_ubound = col - int(np.ceil(gaplens[row-1]/2)) + 1\n",
    "                #if crange_lbound >= crange_ubound:\n",
    "                #    print(f'crange_lb = {crange_lbound}, crange_ubound = {crange_ubound}, row = {row}, col = {col}')\n",
    "                if crange_ubound > crange_lbound:\n",
    "                    D[row, col] = np.min(D[row-1, crange_lbound:crange_ubound]) + C[row,col]\n",
    "                    B[row, col] = col - (crange_lbound + np.argmin(D[row-1, crange_lbound:crange_ubound]))\n",
    "    \n",
    "    path = dtw_backtrace_sparse(D, B, gaplens, steps, subseq=True)\n",
    "    path.reverse()\n",
    "    path = np.array(path).T\n",
    "\n",
    "    return D, B, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbadb1d6-b24e-4763-9238-188f10a85b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def dtw_backtrace_sparse(D, B, gaplens, steps, subseq):\n",
    "    '''\n",
    "    Backtraces through the cumulative cost matrix D\n",
    "    \n",
    "    Inputs:\n",
    "        D: cumulative cost matrix\n",
    "        B: backtrace matrix\n",
    "        gaplens: array specifying the gap lengths between selected features\n",
    "        steps: a numpy matrix specifying the allowable transitions.  It should be of dimension (L, 2), where each row specifies (row step, col step)\n",
    "        subseq: boolean indicating whether to assume a subsequence alignment\n",
    "    \n",
    "    Returns:\n",
    "        A numpy array of (row, col) coordinates for the optimal path.\n",
    "    '''\n",
    "\n",
    "    rstart = B.shape[0] - 1\n",
    "    if subseq:\n",
    "        cstart = np.argmin(D[-1])\n",
    "    else:\n",
    "        cstart = B.shape[1] - 1\n",
    "    pos = (rstart, cstart)\n",
    "    path = []\n",
    "    path.append(pos)\n",
    "    while (pos[0] != 0 and pos[1] != 0) or (pos[0] and subseq):\n",
    "        \n",
    "        (row, col) = pos\n",
    "        if row >= 2 and gaplens[row-1] == 1 and gaplens[row-2] == 1:\n",
    "            \n",
    "            # dense matching\n",
    "            stepidx = B[row, col]\n",
    "            (rstep, cstep) = steps[stepidx]\n",
    "            pos = (row-rstep, col-cstep)\n",
    "            path.append(pos)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # sparse matching\n",
    "            rstep = 1\n",
    "            cstep = B[row, col]\n",
    "            pos = (row-rstep, col-cstep)\n",
    "            path.append(pos)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75fcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_cache_dir(indir):\n",
    "    '''\n",
    "    Verifies that the specified cache directory has the required files.\n",
    "    \n",
    "    Inputs\n",
    "    indir: The cache directory to verify\n",
    "    '''\n",
    "\n",
    "    # Feature Files\n",
    "    assert os.path.exists(f'{indir}/o_chroma.npy'), f'Missing o_chroma.npy in {indir}'\n",
    "    assert os.path.exists(f'{indir}/o_mfcc.npy'), f'Missing o_mfcc.npy in {indir}'\n",
    "    assert os.path.exists(f'{indir}/po_chroma.npy'), f'Missing po_chroma.npy in {indir}'\n",
    "    assert os.path.exists(f'{indir}/po_mfcc.npy'), f'Missing po_mfcc.npy in {indir}'\n",
    "    assert os.path.exists(f'{indir}/gmm_O.pkl'), f'Missing gmm_O.pkl in {indir}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "588ee8df",
   "metadata": {},
   "source": [
    "## Online Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cedd6f21",
   "metadata": {},
   "source": [
    "In the online processing stage, we do two things:\n",
    "1. compute an offline alignment between the piano and estimated piano using DTW,\n",
    "2. use the predicted alignment to infer the alignment between the piano and orchestra recordings\n",
    "\n",
    "Note that this implementation is an offline system, but is implemented in a way that can be extended to the online case easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_processing(scenario_dir, out_dir, cache_dir, hop_length, steps, weights):\n",
    "    '''\n",
    "    Carries out `online' processing using the MATCH algorithm.\n",
    "    \n",
    "    Inputs\n",
    "    scenario_dir: The scenario directory to process\n",
    "    out_dir: The directory to put results, intermediate files, and logging info\n",
    "    cache_dir: The cache directory\n",
    "    hop_sec: The hop size in sec used in the offline DTW stage\n",
    "    separation_dir: directory where the pre-separated audio files are stored\n",
    "\n",
    "    This function will compute and save the predicted alignment in the output directory in a file hyp.npy\n",
    "    '''\n",
    "    # TODO: move this below `os.makedirs` and do the separation here instead of importing pre-separated files\n",
    "    piece_name = Path(cache_dir).name.split('_')\n",
    "    piece_name.pop(2)\n",
    "    piece_name = '_'.join(piece_name) # e.g. rach2_mov1\n",
    "    #verify_separated_file(separation_dir, piece_name)\n",
    "    \n",
    "    # verify & setup\n",
    "    # System_MATCH.verify_match_installation()\n",
    "    system_utils.verify_scenario_dir(scenario_dir)\n",
    "    verify_cache_dir(cache_dir)\n",
    "    assert not os.path.exists(out_dir), f'Output directory {out_dir} already exists.'\n",
    "    os.makedirs(out_dir)\n",
    "           \n",
    "    # compute P features\n",
    "    p_file = f'{scenario_dir}/p.wav'\n",
    "    y, sr = lb.core.load(p_file)\n",
    "    F_p_chroma = lb.feature.chroma_cqt(y=y, sr=sr, hop_length=hop_length, norm=2)\n",
    "    F_p_mfcc = lb.feature.mfcc(y=y, sr=sr, hop_length=hop_length)    \n",
    "    hop_sec = hop_length / sr\n",
    "\n",
    "    # load O and PO features\n",
    "    F_o_chroma = np.load(f'{cache_dir}/o_chroma.npy')\n",
    "    F_o_mfcc = np.load(f'{cache_dir}/o_mfcc.npy')\n",
    "    F_po_chroma = np.load(f'{cache_dir}/po_chroma.npy')\n",
    "    F_po_mfcc = np.load(f'{cache_dir}/po_mfcc.npy')\n",
    "\n",
    "    # select matching portion of PO\n",
    "    C = align_tools.cosine_dist(F_p_chroma, F_po_chroma)\n",
    "    _, _, wp_AB = dtw.dtw(C, steps, weights, True) # P - PO subsequence alignment\n",
    "    po_match_start_frm, po_match_end_frm = wp_AB[1,0], wp_AB[1,-1] + 1\n",
    "    F_po_match_mfcc = F_po_mfcc[:,po_match_start_frm:po_match_end_frm]\n",
    "    F_po_match_chroma = F_po_chroma[:,po_match_start_frm:po_match_end_frm]\n",
    "\n",
    "    # load GMM model for orchestra MFCCs\n",
    "    with open(f'{cache_dir}/gmm_O.pkl', 'rb') as f:\n",
    "        gmm_O = pickle.load(f)\n",
    "\n",
    "    # fit GMM model to piano MFCCs\n",
    "    gmm_P = mixture.GaussianMixture(n_components=10, covariance_type='diag', random_state=0).fit(F_p_mfcc[1:13,:].T)\n",
    "\n",
    "    # classify PO_match frames\n",
    "    gmm_O_scores = gmm_O.score_samples(F_po_match_mfcc[1:13,:].T)\n",
    "    gmm_P_scores = gmm_P.score_samples(F_po_match_mfcc[1:13,:].T)\n",
    "    idx_sel_P = np.where(gmm_P_scores > gmm_O_scores)[0]\n",
    "    gaplens_P = idx_sel_P[1:] - idx_sel_P[0:-1]\n",
    "    gaplens_P = np.append(gaplens_P, len(gmm_P_scores) - idx_sel_P[-1])\n",
    "    idx_sel_O = np.where(gmm_P_scores <= gmm_O_scores)[0]\n",
    "    gaplens_O = idx_sel_O[1:] - idx_sel_O[0:-1]\n",
    "    gaplens_O = np.append(gaplens_O, len(gmm_P_scores) - idx_sel_O[-1])\n",
    "\n",
    "    # compute dense-sparse alignment for PO_match - O\n",
    "    C = 1 - F_po_match_chroma[:,idx_sel_O].T @ F_o_chroma\n",
    "    D, B, wp_BC = dtw_sparse_subseq(C, gaplens_O) # PO_match - O alignment\n",
    "    wp_BC[0,:] = idx_sel_O[wp_BC[0,:]] # convert back to PO frames\n",
    "    wp_BC[0,:] = wp_BC[0,:] + po_match_start_frm # account for offset, specifies PO - O alignment\n",
    "\n",
    "    # re-estimate PO - P alignment\n",
    "    C = 1 - F_po_match_chroma[:,idx_sel_P].T @ F_p_chroma\n",
    "    D, B, wp_AB_reest = dtw_sparse_subseq(C, gaplens_P)\n",
    "    wp_AB_reest[0,:] = idx_sel_P[wp_AB_reest[0,:]] # convert back to PO frames\n",
    "    wp_AB_reest[0,:] = wp_AB_reest[0,:] + po_match_start_frm # account for offset\n",
    "    wp_AB_reest = np.flipud(wp_AB_reest) # now specifies P - PO alignment\n",
    "\n",
    "    # infer piano-orchestra alignment\n",
    "    wp_AC = align_tools.infer_alignment(wp_AB_reest, wp_BC, frames=True)\n",
    "    np.save(f'{out_dir}/hyp.npy', wp_AC*hop_sec)\n",
    "    np.save(f'{out_dir}/o_po_align.npy', np.flipud(wp_BC))\n",
    "    np.save(f'{out_dir}/p_po_align.npy', wp_AB_reest)\n",
    "    #np.save(f'{out_dir}/runtime_p_po.npy', t_end - t_start)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e53b7a-7140-4987-9c89-4e0bf32899b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectFeatures(F, frac_keep):\n",
    "    '''\n",
    "    Selects a subset of features that have the highest flux.\n",
    "\n",
    "    Inputs:\n",
    "        F: feature matrix of size DxN, where D is the feature dimension and N is the number of features\n",
    "        frac_keep: the fraction of features to keep, a scalar between 0 and 1\n",
    "\n",
    "    Returns:\n",
    "        F_sel: a DxM feature matrix containing the selected subset of features\n",
    "        idx_sel: an array of length M specifying the indices of the features that were selected\n",
    "        gaplens: an array of length M specifying the gap lengths between selected features\n",
    "        flux_thresh: the threshold used to select features based on their flux\n",
    "    '''\n",
    "    flux_vals = np.sum(np.abs(F[:,0:-1] - F[:,1:]), axis=0)\n",
    "    flux_thresh = sorted(flux_vals, reverse=True)[int(np.round(frac_keep * len(flux_vals)))-1]\n",
    "    idx_sel = np.where(np.array(flux_vals > flux_thresh) == 1)[0]\n",
    "    gaplens = idx_sel[1:] - idx_sel[0:-1]\n",
    "    gaplens = np.append(gaplens, len(flux_vals) - idx_sel[-1])\n",
    "    F_sel = F[:,idx_sel]\n",
    "    \n",
    "    return F_sel, idx_sel, gaplens, flux_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_hyp_dir(indir):\n",
    "    '''\n",
    "    Verifies that the specified scenario hypothesis directory has the required files.\n",
    "    \n",
    "    Inputs\n",
    "    indir: The cache directory to verify\n",
    "    '''\n",
    "    assert os.path.exists(f'{indir}/hyp.npy'), f'{indir} is missing the required files, please re run the online processing'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b28c8bb",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cd9f6c3",
   "metadata": {},
   "source": [
    "Here is an example of how to call the offline and online processing functions on a scenario directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario_dir = 'scenarios/s2'\n",
    "# out_dir = 'experiments/test/s2'\n",
    "# cache_dir = 'experiments/test/cache'\n",
    "# hop_size = 512\n",
    "# steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "# weights = np.array([2,3,3], dtype=np.float64)\n",
    "# offline_processing(scenario_dir, cache_dir, hop_size, steps, weights)\n",
    "# online_processing(scenario_dir, out_dir, cache_dir, hop_size, steps, weights, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcda1f-1a06-4c31-a1cb-0bad8795d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1571e-4963-4a68-9756-b6b6b63c2eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario_dir = 'scenarios/s2'\n",
    "# #out_dir = 'experiments/test/s2'\n",
    "# #cache_dir = 'experiments/test/cache'\n",
    "# hop_length = 512\n",
    "# #steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "# #weights = np.array([2,3,3], dtype=np.float64)\n",
    "# #offline_processing(scenario_dir, cache_dir, hop_size, steps, weights)\n",
    "# #online_processing(scenario_dir, out_dir, cache_dir, hop_size, steps, weights, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bb4f23-4e3f-4cf7-bcf5-c52da86eb50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute orchestra features\n",
    "# o_file = f'{scenario_dir}/o.wav'\n",
    "# y_o, sr = lb.core.load(o_file)\n",
    "# F_o_chroma = lb.feature.chroma_cqt(y=y_o, sr=sr, hop_length=hop_length, norm=2) \n",
    "# F_o_mfcc = lb.feature.mfcc(y=y_o, sr=sr, hop_length=hop_length)\n",
    "\n",
    "# # compute full mix features\n",
    "# po_file = f'{scenario_dir}/po.wav'\n",
    "# y_po, sr = lb.core.load(po_file)\n",
    "# F_po_chroma = lb.feature.chroma_cqt(y=y_po, sr=sr, hop_length=hop_length, norm=2)\n",
    "# F_po_mfcc = lb.feature.mfcc(y=y_po, sr=sr, hop_length=hop_length)\n",
    "\n",
    "# # compute piano features\n",
    "# p_file = f'{scenario_dir}/p.wav'\n",
    "# y_p, sr = lb.core.load(p_file)\n",
    "# F_p_chroma = lb.feature.chroma_cqt(y=y_p, sr=sr, hop_length=hop_length, norm=2)\n",
    "# F_p_mfcc = lb.feature.mfcc(y=y_p, sr=sr, hop_length=hop_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11cbb0-52cf-419e-9af8-97c929ecb256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # select matching portion of PO\n",
    "# steps = np.array([[1,1],[1,2],[2,1]])\n",
    "# weights = np.array([1,1,2])\n",
    "# C = align_tools.cosine_dist(F_p_chroma, F_po_chroma)\n",
    "# _, _, wp_AB = dtw.dtw(C, steps, weights, True)\n",
    "# po_match_start_frm, po_match_end_frm = wp_AB[1,0], wp_AB[1,-1] + 1\n",
    "# F_po_match_mfcc = F_po_mfcc[:,po_match_start_frm:po_match_end_frm]\n",
    "# F_po_match_chroma = F_po_chroma[:,po_match_start_frm:po_match_end_frm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7970ec-6b82-4008-88c5-01a5d7437dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit GMM model to orchestra MFCCs\n",
    "# #F_o_mfcc_scaled = sklearn.preprocessing.scale(F_o_mfcc, axis=1)\n",
    "# #gmm_O = mixture.GaussianMixture(n_components=10, covariance_type='diag', reg_covar = 1e-2, random_state=0).fit(F_o_mfcc[1:,:].T)\n",
    "# gmm_O = mixture.GaussianMixture(n_components=10, covariance_type='diag', random_state=None).fit(F_o_mfcc[1:13,:].T)\n",
    "# #gmm_O2 = mixture.GaussianMixture(n_components=3, covariance_type='diag', random_state=None).fit(F_o_mfcc[1:13,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e123e84c-0768-4d8f-a513-ea08f4ae2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit GMM model to piano MFCCs\n",
    "# #F_p_mfcc_scaled = sklearn.preprocessing.scale(F_p_mfcc, axis=1)\n",
    "# #gmm_P = mixture.GaussianMixture(n_components=10, covariance_type='diag', reg_covar = 1e-2, random_state=0).fit(F_p_mfcc.T)\n",
    "# gmm_P = mixture.GaussianMixture(n_components=10, covariance_type='diag', random_state=None).fit(F_p_mfcc[1:13,:].T)\n",
    "# #gmm_P2 = mixture.GaussianMixture(n_components=3, covariance_type='diag', random_state=None).fit(F_p_mfcc[1:13,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d1831c-35fb-4815-b14d-5b080941c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmm_O_scores = gmm_O.score_samples(F_po_match_mfcc[1:13,:].T)\n",
    "# gmm_P_scores = gmm_P.score_samples(F_po_match_mfcc[1:13,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb931a-517d-401a-83c8-2a32bff2f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gmm_O_scores2 = gmm_O2.score_samples(F_po_mfcc[1:13,:].T)\n",
    "#gmm_P_scores2 = gmm_P2.score_samples(F_po_mfcc[1:13,:].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b594902-9b7c-4426-986f-4b4fb5245555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_sel_P = np.where(gmm_P_scores > gmm_O_scores)[0]\n",
    "# gaplens_P = idx_sel_P[1:] - idx_sel_P[0:-1]\n",
    "# gaplens_P = np.append(gaplens_P, len(gmm_P_scores) - idx_sel_P[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a84cd-d828-4f79-882d-e0827498e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_sel_O = np.where(gmm_P_scores <= gmm_O_scores)[0]\n",
    "# gaplens_O = idx_sel_O[1:] - idx_sel_O[0:-1]\n",
    "# gaplens_O = np.append(gaplens_O, len(gmm_P_scores) - idx_sel_O[-1])\n",
    "\n",
    "# #return idx_sel_P, gaplens_P, idx_sel_O, gaplens_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a5578-7576-4167-92b5-04ad1006d4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute dense-sparse alignment for PO_match - O\n",
    "# C = 1 - F_po_match_chroma[:,idx_sel_O].T @ F_o_chroma\n",
    "# D, B, wp = dtw_sparse_subseq(C, gaplens_O)\n",
    "# wp[0,:] = idx_sel_O[wp[0,:]] # convert back to PO frames\n",
    "# wp[0,:] = wp[0,:] + po_match_start_frm # account for offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f728e-244f-470b-aaf7-c308b4c0317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compare regular PO-O subseq DTW alignment path\n",
    "# C2 = 1 - F_po_match_chroma.T @ F_o_chroma\n",
    "# _, _, wp2 = dtw.dtw(C2, steps, weights, True)\n",
    "# wp2[0,:] = wp2[0,:] + po_match_start_frm # account for offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5148358-cb9c-476f-991a-4b79e62e1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(wp[0,:],wp[1,:], 'r.')\n",
    "# #plt.plot(wp2[0,:],wp2[1,:], 'b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1323bb94-f803-4913-9aab-26c109993a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute dense-sparse alignment for PO_match - P\n",
    "# C3 = 1 - F_po_match_chroma[:,idx_sel_P].T @ F_p_chroma\n",
    "# D, B, wp3 = dtw_sparse_subseq(C3, gaplens_P)\n",
    "# wp3[0,:] = idx_sel_P[wp3[0,:]] # convert back to PO frames\n",
    "# wp3[0,:] = wp3[0,:] + po_match_start_frm # account for offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2520b3e-5b76-4f42-850b-e4f881b690fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(wp3[0,:],wp3[1,:], 'r.') # wp3 specifices PO-P\n",
    "# plt.plot(wp_AB[1,:],wp_AB[0,:], 'b') # wp_AB specific P-PO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b881d2-b88c-46a3-98cf-ce28d72faeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getMaxLength(pred):\n",
    "#     maxlen = 0\n",
    "#     maxEndIdx = -1\n",
    "#     curlen = 0\n",
    "#     prevp = -1\n",
    "#     for i, p in enumerate(pred):\n",
    "#         if prevp == p:\n",
    "#             curlen += 1\n",
    "#         else:\n",
    "#             if curlen > maxlen:\n",
    "#                 maxlen = curlen\n",
    "#                 maxEndIdx = i\n",
    "#             curlen = 1\n",
    "#             prevp = p\n",
    "#     if curlen > maxlen:\n",
    "#         maxlen = curlen\n",
    "#         maxEndIdx = -1\n",
    "#     return maxlen, maxEndIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94540e9d-2a10-4722-a216-553fdc151359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PianoConcertoAccompaniment",
   "language": "python",
   "name": "pianoconcertoaccompaniment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
