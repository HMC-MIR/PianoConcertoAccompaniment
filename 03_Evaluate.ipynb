{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db0bb1e9",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6849e1b",
   "metadata": {},
   "source": [
    "This notebook evaluates the quality of the online alignments in a given experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c3ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import import_ipynb\n",
    "import system_utils\n",
    "import eval_tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91986a9a",
   "metadata": {},
   "source": [
    "## Calculate Alignment Errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f50141f0",
   "metadata": {},
   "source": [
    "First we calculate the alignment errors of a given system on all evaluated measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da968c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = 'experiments/PairwiseSparseDTW_0.8' # change\n",
    "scenarios_dir = 'scenarios'\n",
    "eval_dir = 'eval/' + os.path.basename(exp_dir)\n",
    "eval_tools.calcAlignErrors_batch(exp_dir, scenarios_dir, eval_dir, '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c341afa4",
   "metadata": {},
   "source": [
    "## Plot Error vs Tolerance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "484f7bcc",
   "metadata": {},
   "source": [
    "We can visualize the results by plotting the error rate across a range of error tolerances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotErrorVsTolerance(eval_dirs, maxTol, savefile = None, style='bar', bar_tols=[100, 200, 500, 1000, 2000]):\n",
    "    '''\n",
    "    Plots the error rate across a range of error tolerances.\n",
    "    \n",
    "    Inputs\n",
    "    eval_dir: the eval directories to plot\n",
    "    maxTol: maximum error tolerance to consider (in milliseconds)\n",
    "    savefile: if specified, will save the figure to the given filepath\n",
    "    style: 'bar' or 'line'\n",
    "    tols: list of error tolerances to plot if using 'bar'\n",
    "    '''\n",
    "    \n",
    "    errRates_list = []\n",
    "    #color=['blue','orange','green','lightgreen','red'] # hard coded for main results figure\n",
    "    for i, eval_dir in enumerate(eval_dirs):\n",
    "    \n",
    "        # load\n",
    "        with open(f'{eval_dir}/errs.pkl', 'rb') as f:\n",
    "            d = pickle.load(f)\n",
    "\n",
    "        # flattened list\n",
    "        errs = []\n",
    "        for scenario_id in d:\n",
    "            errs = np.append(errs, d[scenario_id][0])\n",
    "\n",
    "        # calculate error rates\n",
    "        errRates = np.zeros(maxTol+1)\n",
    "        tols = np.arange(maxTol+1)\n",
    "        for j in tols:\n",
    "            errRates[j] = np.mean(np.abs(errs) > j/1000)\n",
    "        errRates_list.append(errRates)\n",
    "        \n",
    "        if style == 'line':\n",
    "            plt.plot(tols, errRates * 100.0)\n",
    "        elif style == 'bar':\n",
    "            bar_width = 0.1\n",
    "            errs = [errRates[tol] * 100.0 for tol in bar_tols]\n",
    "            pos = np.arange(len(errs)) + i * bar_width\n",
    "            # hard code for main results figure\n",
    "            #plt.bar(pos, errs, width=bar_width, label=os.path.basename(eval_dir), color = color[i])\n",
    "            plt.bar(pos, errs, width=bar_width, label=os.path.basename(eval_dir))\n",
    "            plt.xticks([r + bar_width*len(eval_dirs)/2 for r in range(len(bar_tols))], map(str, bar_tols))\n",
    "        \n",
    "    plt.ylabel('Error Rate (%)')\n",
    "    plt.xlabel('Error Tolerance (ms)')\n",
    "    plt.legend([os.path.basename(eval_dir) for eval_dir in eval_dirs])\n",
    "    plt.grid(linestyle='--')\n",
    "    if savefile:\n",
    "        plt.savefig(savefile)\n",
    "\n",
    "    return errRates_list, tols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abf87a9b",
   "metadata": {},
   "source": [
    "Plot the error rate vs error tolerance curve for one system of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTol = 5000 # in milliseconds\n",
    "eval_dir = 'eval/PairwiseSparseDTW_0.8'\n",
    "errRates_list, tols = plotErrorVsTolerance([eval_dir], maxTol, savefile=False)\n",
    "for i in [100,200,500,1000,2000]:\n",
    "    print(errRates_list[0][i]*100.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d180c93",
   "metadata": {},
   "source": [
    "Overlay multiple error curves for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "systems_to_compare = ['NaivePairwiseDTW', 'ISA','SeparatedDTW_Spleeter','SeparatedDTW_HDemucs', 'PairwiseSparseDTW_0.8']\n",
    "eval_dirs = [f'eval/{s}' for s in systems_to_compare]\n",
    "errRates_list, tols = plotErrorVsTolerance(eval_dirs, maxTol, savefile='Results.png')\n",
    "[errRates_list[i][500]*100.0 for i in range(len(eval_dirs))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e209e5f7",
   "metadata": {},
   "source": [
    "## Separate error curves by condition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be19717b",
   "metadata": {},
   "source": [
    "Visualize the same error curve for a single system, but separated by different conditions.  For example, one can visualize the performance across:\n",
    "- TSM factor\n",
    "- full mix recording\n",
    "- concerto\n",
    "- composer\n",
    "- chunk within a movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotErrorVsTolerance_separated(eval_dir, mapping, maxTol, savefile = None):\n",
    "    '''\n",
    "    Plots error rate across a range of error tolerances.  Data is separated into categories\n",
    "    specified in the given dictionary, and each category is plotted as a separate curve.\n",
    "    \n",
    "    Inputs\n",
    "    eval_dir: the eval directory to process\n",
    "    mapping: a dictionary whose key is the scenario id and whose value is the category name.\n",
    "      Any scenario ids that are not in the dictionary will be excluded from the plot.\n",
    "    maxTol: maximum error tolerance to consider (in milliseconds)\n",
    "    savefile: if specified, will save the figure to the given filepath\n",
    "    '''\n",
    "    \n",
    "    # initialize\n",
    "    categories = list(sorted(set(mapping.values())))\n",
    "    errors_by_category = {}\n",
    "    for c in categories:\n",
    "        errors_by_category[c] = [] # flattened list of alignment errors by category\n",
    "    \n",
    "    # load\n",
    "    with open(f'{eval_dir}/errs.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)  # key: scenario_id, value: (errors, measureNums)\n",
    "\n",
    "    # aggregate data by category\n",
    "    for scenario_id in d:\n",
    "        if scenario_id in mapping:\n",
    "            category = mapping[scenario_id]\n",
    "            errors_by_category[category] = np.append(errors_by_category[category], d[scenario_id][0])\n",
    "\n",
    "    # calculate error rates by category\n",
    "    errRates_list = {}\n",
    "    numPts = {}\n",
    "    for c in categories:\n",
    "        errRates = np.zeros(maxTol+1)\n",
    "        tols = np.arange(maxTol+1)\n",
    "        for i in tols:\n",
    "            errRates[i] = np.mean(np.abs(errors_by_category[c]) > i/1000)\n",
    "        errRates_list[c] = errRates\n",
    "        numPts[c] = len(errors_by_category[c]) # for debugging\n",
    "        plt.plot(tols, errRates * 100.0)\n",
    "        \n",
    "    plt.ylabel('Error Rate (%)')\n",
    "    plt.xlabel('Error Tolerance (ms)')\n",
    "    plt.legend(categories)\n",
    "    plt.grid(linestyle='--')\n",
    "    if savefile:\n",
    "        plt.savefig(savefile)\n",
    "\n",
    "    return errRates_list, tols, numPts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapByTSMFactor():\n",
    "    '''\n",
    "    Constructs a mapping separated by TSM factor.\n",
    "    '''\n",
    "    d = system_utils.get_scenario_info(SCENARIOS_SUMMARY)\n",
    "    mapping = {}\n",
    "    for scenario_id in d:\n",
    "        mapping[scenario_id] = d[scenario_id]['p'].split('/')[-2] # e.g. 'tsm0.80'\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d39993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapByFullMix():\n",
    "    '''\n",
    "    Constructs a mapping separated by full mix recording.\n",
    "    '''\n",
    "    d = system_utils.get_scenario_info(SCENARIOS_SUMMARY)\n",
    "    mapping = {}\n",
    "    for scenario_id in d:\n",
    "        mapping[scenario_id] = os.path.splitext(os.path.basename(d[scenario_id]['po']))[0] # e.g. 'rach2_mov1_PO1'\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b09863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapByComposer():\n",
    "    '''\n",
    "    Constructs a mapping separated by composer.\n",
    "    '''\n",
    "    d = system_utils.get_scenario_info(SCENARIOS_SUMMARY)\n",
    "    mapping = {}\n",
    "    for scenario_id in d:\n",
    "        po_id = os.path.splitext(os.path.basename(d[scenario_id]['po']))[0] # e.g. 'rach2_mov1_PO1'\n",
    "        concerto_id = po_id.split('_')[0] # e.g. 'rach2'\n",
    "        composer = re.search(r'([a-z]+)\\d+', concerto_id).group(1)\n",
    "        mapping[scenario_id] = composer\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapByChunk(mov_id):\n",
    "    '''\n",
    "    Constructs a mapping separated by chunk within a specified concerto movement.\n",
    "    \n",
    "    Inputs\n",
    "    mov_id: id specifying the concerto movement to analyze, e.g. 'rach2_mov1'\n",
    "    '''\n",
    "    # construct mapping with tuple categories\n",
    "    d = system_utils.get_scenario_info(SCENARIOS_SUMMARY)\n",
    "    mapping = {}\n",
    "    for scenario_id in d:\n",
    "        if mov_id in d[scenario_id]['po']: # only keep scenario ids for the concerto movement of interest\n",
    "            mapping[scenario_id] = (d[scenario_id]['measStart'], d[scenario_id]['measEnd'])\n",
    "        \n",
    "    # map tuples to string (e.g. 'chunk1', 'chunk2'\n",
    "    tup2str = {}\n",
    "    for i, tup in enumerate(sorted(set(mapping.values()))):\n",
    "        tup2str[tup] = f'Chunk{i+1}'\n",
    "\n",
    "    # construct mapping with string categories\n",
    "    renamed = {}\n",
    "    for scenario_id in mapping:\n",
    "        renamed[scenario_id] = tup2str[mapping[scenario_id]]\n",
    "        \n",
    "    return renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = 'eval/PairwiseSparseDTW_0.8'\n",
    "maxTol = 2000 # in milliseconds\n",
    "SCENARIOS_SUMMARY = 'scenarios/scenarios.summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de2fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates_list, tols, numPts = plotErrorVsTolerance_separated(eval_dir, mapByTSMFactor(), maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates_list, tols, numPts = plotErrorVsTolerance_separated(eval_dir, mapByFullMix(), maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccafd65-4f3d-4d26-ade3-0e38b6cf4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates_list, tols, numPts = plotErrorVsTolerance_separated(eval_dir, mapByChunk('beeth1_mov1'), maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd320a1-b21e-4111-8cc4-3f08d6d77e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates_list, tols, numPts = plotErrorVsTolerance_separated(eval_dir, mapByComposer(), maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729a828",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [100,200,500,1000,2000]:\n",
    "    print(errRates_list['bach'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fae71b-8e1b-40e1-adf3-36f31e0d6b63",
   "metadata": {},
   "source": [
    "## Separate results by P-PO and O-PO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce16b0c-29c8-4eb8-92a1-b7202aa2561b",
   "metadata": {},
   "source": [
    "The accuracy of the P - O alignment depends on two separate alignments: the P - PO and O - PO alignments.  The code below characterizes the alignment accuracy of each of these two alignments separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a0d188-d93d-4464-9145-f0c0c02610c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAlignErrors_batch_P_PO(exp_dir, scenarios_dir, annot_dir, out_dir):\n",
    "    '''\n",
    "    Calculates the P-PO alignment errors for all scenarios in an experiment directory for which PO beat annotations exist.\n",
    "    This function is a modification of calcAlignErrors_batch(). \n",
    "    \n",
    "    Inputs\n",
    "    exp_dir: the experiment directory to evaluate\n",
    "    scenarios_dir: the directory containing the scenarios information\n",
    "    annot_dir: the directory containing beat annotation files\n",
    "    out_dir: the directory to save outputs and figures to\n",
    "    '''\n",
    "    # only evaluate scenarios whose PO file has beat annotations\n",
    "    d = {}\n",
    "    for scenario_id in eval_tools.getScenarioIds(scenarios_dir):\n",
    "        hypFile = f'{exp_dir}/{scenario_id}/p_po_align.npy'\n",
    "        pianoAnnot = f'{scenarios_dir}/{scenario_id}/p.beats'\n",
    "        orchAnnot = f'{scenarios_dir}/{scenario_id}/o.beats'\n",
    "        scenarioInfo = f'{scenarios_dir}/{scenario_id}/scenario.info'\n",
    "        mixAudiofile = system_utils.get_scenario_info(scenarioInfo)['po'] # e.g. basedir/audio/rach2_mov1_PO1.wav\n",
    "        mixAnnot = f'{annot_dir}/{Path(mixAudiofile).stem}.beats'\n",
    "        if not os.path.exists(mixAnnot):\n",
    "            continue # skip if no PO annotation file\n",
    "        errs, measNums = eval_tools.calcAlignErrors_single(hypFile, pianoAnnot, mixAnnot, scenarioInfo, frames=True)\n",
    "        d[scenario_id] = (errs, measNums) # key: scenario_id, value: (errors, measureNums)\n",
    "        \n",
    "    # save\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    outfile = f'{out_dir}/errs.pkl'\n",
    "    pickle.dump(d, open(outfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7d88b-9c23-4ed8-bc7a-55736aaa09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAlignErrors_batch_O_PO(exp_dir, scenarios_dir, annot_dir, out_dir, in_cache = True):\n",
    "    '''\n",
    "    Calculates the O-PO alignment errors for all scenarios in an experiment directory for which PO beat annotations exist.\n",
    "    This function is a modification of calcAlignErrors_batch(). \n",
    "    \n",
    "    Inputs\n",
    "    exp_dir: the experiment directory to evaluate\n",
    "    scenarios_dir: the directory containing the scenarios information\n",
    "    annot_dir: the directory containing beat annotation files\n",
    "    out_dir: the directory to save outputs and figures to\n",
    "    in_cache: if True, expects the alignment file to be in the cache folder\n",
    "    '''\n",
    "    # only evaluate scenarios whose PO file has beat annotations\n",
    "    d = {}\n",
    "    for scenario_id in eval_tools.getScenarioIds(scenarios_dir):\n",
    "        orchAnnot = f'{scenarios_dir}/{scenario_id}/o.beats'\n",
    "        scenarioInfo = f'{scenarios_dir}/{scenario_id}/scenario.info'\n",
    "        mixAudiofile = system_utils.get_scenario_info(scenarioInfo)['po'] # e.g. basedir/audio/rach2_mov1_PO1.wav\n",
    "        basename = Path(mixAudiofile).stem # rach2_mov1_PO1\n",
    "        mixAnnot = f'{annot_dir}/{basename}.beats'\n",
    "        if not os.path.exists(mixAnnot):\n",
    "            continue # skip if no PO annotation file       \n",
    "        if in_cache:\n",
    "            hypFile = f'{exp_dir}/cache/{getCacheDir(basename)}/o_po_align.npy'        \n",
    "        else:\n",
    "            hypFile = f'{exp_dir}/{scenario_id}/o_po_align.npy'\n",
    "        errs, measNums = eval_tools.calcAlignErrors_single(hypFile, orchAnnot, mixAnnot, scenarioInfo, frames=True)\n",
    "        d[scenario_id] = (errs, measNums) # key: scenario_id, value: (errors, measureNums)\n",
    "        \n",
    "    # save\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    outfile = f'{out_dir}/errs.pkl'\n",
    "    pickle.dump(d, open(outfile, 'wb'))\n",
    "\n",
    "def getCacheDir(basename):\n",
    "    '''\n",
    "    Given the basename of the mix audio file, determine the cache directory name.\n",
    "    '''\n",
    "    parts = basename.split('_') # ['rach2', 'mov1', 'PO1']\n",
    "    parts.insert(-1, 'O1')\n",
    "    return '_'.join(parts) # rach2_mov1_O1_PO1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396c4d1f-2b20-4ecb-b9c1-5d8ec94be91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = 'experiments/PairwiseSparseDTW_0.8' # change\n",
    "scenarios_dir = 'scenarios'\n",
    "annot_dir = 'annot'\n",
    "eval_root = 'backup/eval_p_po'\n",
    "eval_dir = f'{eval_root}/' + os.path.basename(exp_dir)\n",
    "calcAlignErrors_batch_P_PO(exp_dir, scenarios_dir, annot_dir, eval_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bb806-69e3-4be7-a6f1-377c0e5d2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "systems_to_compare = ['PairwiseSparseDTW_0.8']\n",
    "#systems_to_compare = ['NaivePairwiseDTW', 'SeparatedDTW_SPL-PT', 'SeparatedDTW_SPL-TTA','SeparatedDTW_HDemucs', 'ISA_Chroma', 'PairwiseSparseDTW_0.8', 'TimeSparse']\n",
    "eval_dirs = [f'{eval_root}/{s}' for s in systems_to_compare]\n",
    "errRates_list, tols = plotErrorVsTolerance(eval_dirs, maxTol)\n",
    "plt.title('P-PO Alignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad80731f-9465-4c78-8eb1-5f467e0adaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#['NaivePairwiseDTW', 'SeparatedDTW_SPL-PT', 'SeparatedDTW_SPL-TTA','SeparatedDTW_HDemucs', 'ISA_Chroma', 'ISA_CQT', 'ISA_BCQT', 'PairwiseSparseDTW_0.8']\n",
    "exp_dir = 'experiments/PairwiseSparseDTW_0.8' # change\n",
    "scenarios_dir = 'scenarios'\n",
    "annot_dir = 'annot'\n",
    "eval_root = 'backup/eval_o_po'\n",
    "eval_dir = f'{eval_root}/' + os.path.basename(exp_dir)\n",
    "calcAlignErrors_batch_O_PO(exp_dir, scenarios_dir, annot_dir, eval_dir, in_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94b3e1-8fdc-4492-9ee7-e74a17ee58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "systems_to_compare = ['PairwiseSparseDTW_0.8']\n",
    "#systems_to_compare = ['NaivePairwiseDTW', 'SeparatedDTW_SPL-PT', 'SeparatedDTW_SPL-TTA','SeparatedDTW_HDemucs','PairwiseSparseDTW_0.8', 'TimeSparse']\n",
    "eval_dirs = [f'{eval_root}/{s}' for s in systems_to_compare]\n",
    "errRates_list, tols = plotErrorVsTolerance(eval_dirs, maxTol)\n",
    "plt.title('O-PO Alignment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc37b1-04c0-489e-831c-c6bd64f90c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_dir = 'experiments/ISA_Chroma' # change\n",
    "# scenarios_dir = 'scenarios'\n",
    "# annot_dir = 'annot'\n",
    "# eval_dir = 'eval_o_po/' + os.path.basename(exp_dir)\n",
    "# scenario_id = 's21'\n",
    "# in_cache = False\n",
    "\n",
    "# orchAnnot = f'{scenarios_dir}/{scenario_id}/o.beats'\n",
    "# scenarioInfo = f'{scenarios_dir}/{scenario_id}/scenario.info'\n",
    "# mixAudiofile = system_utils.get_scenario_info(scenarioInfo)['po'] # e.g. basedir/audio/rach2_mov1_PO1.wav\n",
    "# basename = Path(mixAudiofile).stem # rach2_mov1_PO1\n",
    "# mixAnnot = f'{annot_dir}/{basename}.beats'\n",
    "\n",
    "# if in_cache:\n",
    "#     hypFile = f'{exp_dir}/cache/{getCacheDir(basename)}/o_po_align.npy'        \n",
    "# else:\n",
    "#     hypFile = f'{exp_dir}/{scenario_id}/o_po_align.npy'\n",
    "\n",
    "# errs, measNums = eval_tools.calcAlignErrors_single(hypFile, orchAnnot, mixAnnot, scenarioInfo, frames=True)\n",
    "# gt, measNums = eval_tools.getGroundTruthTimestamps(orchAnnot, mixAnnot, scenarioInfo) # ground truth\n",
    "# hypalign = np.load(hypFile) # piano-orchestra predicted alignment in sec\n",
    "\n",
    "# if True:\n",
    "#     hypalign = hypalign / (22050/512)\n",
    "# pred = np.interp(gt[:,0], hypalign[0,:], hypalign[1,:])\n",
    "# err = pred - gt[:,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PianoConcertoAccompaniment",
   "language": "python",
   "name": "pianoconcertoaccompaniment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
