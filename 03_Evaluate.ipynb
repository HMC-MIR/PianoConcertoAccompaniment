{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db0bb1e9",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6849e1b",
   "metadata": {},
   "source": [
    "This notebook evaluates the quality of the online alignments in a given experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c3ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import import_ipynb\n",
    "import system_utils\n",
    "import eval_tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91986a9a",
   "metadata": {},
   "source": [
    "## Calculate Alignment Errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f50141f0",
   "metadata": {},
   "source": [
    "First we calculate the alignment errors of a given system on all evaluated measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da968c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = 'experiments/NaivePairwiseDTW' # change\n",
    "scenarios_dir = 'scenarios'\n",
    "eval_dir = 'eval/' + os.path.basename(exp_dir)\n",
    "eval_tools.calcAlignErrors_batch(exp_dir, scenarios_dir, eval_dir, '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c341afa4",
   "metadata": {},
   "source": [
    "## Plot Error vs Tolerance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "484f7bcc",
   "metadata": {},
   "source": [
    "We can visualize the results by plotting the error rate across a range of error tolerances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotErrorVsTolerance(eval_dirs, maxTol, savefile = None, style='bar', bar_tols=[100, 200, 500, 1000, 2000]):\n",
    "    '''\n",
    "    Plots the error rate across a range of error tolerances.\n",
    "    \n",
    "    Inputs\n",
    "    eval_dir: the eval directories to plot\n",
    "    maxTol: maximum error tolerance to consider (in milliseconds)\n",
    "    savefile: if specified, will save the figure to the given filepath\n",
    "    style: 'bar' or 'line'\n",
    "    tols: list of error tolerances to plot if using 'bar'\n",
    "    '''\n",
    "    \n",
    "    errRates_list = []\n",
    "    for i, eval_dir in enumerate(eval_dirs):\n",
    "    \n",
    "        # load\n",
    "        with open(f'{eval_dir}/errs.pkl', 'rb') as f:\n",
    "            d = pickle.load(f)\n",
    "\n",
    "        # flattened list\n",
    "        errs = []\n",
    "        for scenario_id in d:\n",
    "            errs = np.append(errs, d[scenario_id][0])\n",
    "\n",
    "        # calculate error rates\n",
    "        errRates = np.zeros(maxTol+1)\n",
    "        tols = np.arange(maxTol+1)\n",
    "        for j in tols:\n",
    "            errRates[j] = np.mean(np.abs(errs) > j/1000)\n",
    "        errRates_list.append(errRates)\n",
    "        \n",
    "        if style == 'line':\n",
    "            plt.plot(tols, errRates * 100.0)\n",
    "        elif style == 'bar':\n",
    "            bar_width = 0.1\n",
    "            errs = [errRates[tol] * 100.0 for tol in bar_tols]\n",
    "            pos = np.arange(len(errs)) + i * bar_width\n",
    "            plt.bar(pos, errs, width=bar_width, label=os.path.basename(eval_dir))\n",
    "            plt.xticks([r + bar_width*len(eval_dirs)/2 for r in range(len(bar_tols))], map(str, bar_tols))\n",
    "        \n",
    "    plt.ylabel('Error Rate (%)')\n",
    "    plt.xlabel('Error Tolerance (ms)')\n",
    "    plt.legend([os.path.basename(eval_dir) for eval_dir in eval_dirs])\n",
    "    plt.grid(linestyle='--')\n",
    "    if savefile:\n",
    "        plt.savefig(savefile)\n",
    "\n",
    "    return errRates_list, tols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abf87a9b",
   "metadata": {},
   "source": [
    "Plot the error rate vs error tolerance curve for one system of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTol = 5000 # in milliseconds\n",
    "eval_dir = 'eval/NaivePairwiseDTW'\n",
    "errRates_list, tols = plotErrorVsTolerance([eval_dir], maxTol, savefile=False)\n",
    "errRates_list[0][1000]*100.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d180c93",
   "metadata": {},
   "source": [
    "Overlay multiple error curves for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "systems_to_compare = ['naivePairwiseDTW', 'SeparatedDTW_SPL-PT', 'SeparatedDTW_SPL-TTA', 'SeparatedDTW_HDemucs', 'ISA_chroma', 'ISA_cqt', 'ISA_bcqt']\n",
    "eval_dirs = [f'eval/{s}' for s in systems_to_compare]\n",
    "errRates_list, tols = plotErrorVsTolerance(eval_dirs, maxTol, savefile=False)\n",
    "[errRates_list[i][500]*100.0 for i in range(len(eval_dirs))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e209e5f7",
   "metadata": {},
   "source": [
    "## Separate error curves by condition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be19717b",
   "metadata": {},
   "source": [
    "Visualize the same error curve for a single system, but separated by different conditions.  For example, one can visualize the performance across:\n",
    "- TSM factor\n",
    "- full mix recording\n",
    "- concerto\n",
    "- composer\n",
    "- chunk within a movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotErrorVsTolerance_separated(eval_dir, mapping, maxTol, savefile = None):\n",
    "    '''\n",
    "    Plots error rate across a range of error tolerances.  Data is separated into categories\n",
    "    specified in the given dictionary, and each category is plotted as a separate curve.\n",
    "    \n",
    "    Inputs\n",
    "    eval_dir: the eval directory to process\n",
    "    mapping: a dictionary whose key is the scenario id and whose value is the category name.\n",
    "      Any scenario ids that are not in the dictionary will be excluded from the plot.\n",
    "    maxTol: maximum error tolerance to consider (in milliseconds)\n",
    "    savefile: if specified, will save the figure to the given filepath\n",
    "    '''\n",
    "    \n",
    "    # initialize\n",
    "    categories = list(sorted(set(mapping.values())))\n",
    "    errors_by_category = {}\n",
    "    for c in categories:\n",
    "        errors_by_category[c] = [] # flattened list of alignment errors by category\n",
    "    \n",
    "    # load\n",
    "    with open(f'{eval_dir}/errs.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)  # key: scenario_id, value: (errors, measureNums)\n",
    "\n",
    "    # aggregate data by category\n",
    "    for scenario_id in d:\n",
    "        if scenario_id in mapping:\n",
    "            category = mapping[scenario_id]\n",
    "            errors_by_category[category] = np.append(errors_by_category[category], d[scenario_id][0])\n",
    "\n",
    "    # calculate error rates by category\n",
    "    errRates_list = {}\n",
    "    numPts = {}\n",
    "    for c in categories:\n",
    "        errRates = np.zeros(maxTol+1)\n",
    "        tols = np.arange(maxTol+1)\n",
    "        for i in tols:\n",
    "            errRates[i] = np.mean(np.abs(errors_by_category[c]) > i/1000)\n",
    "        errRates_list[c] = errRates\n",
    "        numPts[c] = len(errors_by_category[c]) # for debugging\n",
    "        plt.plot(tols, errRates * 100.0)\n",
    "        \n",
    "    plt.ylabel('Error Rate (%)')\n",
    "    plt.xlabel('Error Tolerance (ms)')\n",
    "    plt.legend(categories)\n",
    "    plt.grid(linestyle='--')\n",
    "    if savefile:\n",
    "        plt.savefig(savefile)\n",
    "\n",
    "    return errRates_list, tols, numPts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapByTSMFactor():\n",
    "    '''\n",
    "    Constructs a mapping separated by TSM factor.\n",
    "    '''\n",
    "    d = system_utils.get_scenario_info(SCENARIOS_SUMMARY)\n",
    "    mapping = {}\n",
    "    for scenario_id in d:\n",
    "        mapping[scenario_id] = d[scenario_id]['p'].split('/')[-2] # e.g. 'tsm0.80'\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d39993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapByFullMix():\n",
    "    '''\n",
    "    Constructs a mapping separated by full mix recording.\n",
    "    '''\n",
    "    d = system_utils.get_scenario_info(SCENARIOS_SUMMARY)\n",
    "    mapping = {}\n",
    "    for scenario_id in d:\n",
    "        mapping[scenario_id] = os.path.splitext(os.path.basename(d[scenario_id]['po']))[0] # e.g. 'rach2_mov1_PO1'\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b09863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapByComposer():\n",
    "    '''\n",
    "    Constructs a mapping separated by composer.\n",
    "    '''\n",
    "    d = system_utils.get_scenario_info(SCENARIOS_SUMMARY)\n",
    "    mapping = {}\n",
    "    for scenario_id in d:\n",
    "        po_id = os.path.splitext(os.path.basename(d[scenario_id]['po']))[0] # e.g. 'rach2_mov1_PO1'\n",
    "        concerto_id = po_id.split('_')[0] # e.g. 'rach2'\n",
    "        composer = re.search(r'([a-z]+)\\d+', concerto_id).group(1)\n",
    "        mapping[scenario_id] = composer\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapByChunk(mov_id):\n",
    "    '''\n",
    "    Constructs a mapping separated by chunk within a specified concerto movement.\n",
    "    \n",
    "    Inputs\n",
    "    mov_id: id specifying the concerto movement to analyze, e.g. 'rach2_mov1'\n",
    "    '''\n",
    "    # construct mapping with tuple categories\n",
    "    d = system_utils.get_scenario_info(SCENARIOS_SUMMARY)\n",
    "    mapping = {}\n",
    "    for scenario_id in d:\n",
    "        if mov_id in d[scenario_id]['po']: # only keep scenario ids for the concerto movement of interest\n",
    "            mapping[scenario_id] = (d[scenario_id]['measStart'], d[scenario_id]['measEnd'])\n",
    "        \n",
    "    # map tuples to string (e.g. 'chunk1', 'chunk2'\n",
    "    tup2str = {}\n",
    "    for i, tup in enumerate(sorted(set(mapping.values()))):\n",
    "        tup2str[tup] = f'Chunk{i+1}'\n",
    "\n",
    "    # construct mapping with string categories\n",
    "    renamed = {}\n",
    "    for scenario_id in mapping:\n",
    "        renamed[scenario_id] = tup2str[mapping[scenario_id]]\n",
    "        \n",
    "    return renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = 'eval/NaivePairwiseDTW'\n",
    "maxTol = 1000 # in milliseconds\n",
    "SCENARIOS_SUMMARY = 'scenarios/scenarios.summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de2fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates_list, tols, numPts = plotErrorVsTolerance_separated(eval_dir, mapByTSMFactor(), maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates_list, tols, numPts = plotErrorVsTolerance_separated(eval_dir, mapByFullMix(), maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccafd65-4f3d-4d26-ade3-0e38b6cf4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates_list, tols, numPts = plotErrorVsTolerance_separated(eval_dir, mapByChunk('beeth1_mov1'), maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd320a1-b21e-4111-8cc4-3f08d6d77e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates_list, tols, numPts = plotErrorVsTolerance_separated(eval_dir, mapByComposer(), maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729a828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PianoConcertoAccompaniment",
   "language": "python",
   "name": "pianoconcertoaccompaniment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
