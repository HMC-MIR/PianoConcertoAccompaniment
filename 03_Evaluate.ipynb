{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0bb1e9",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6849e1b",
   "metadata": {},
   "source": [
    "This notebook evaluates the quality of the online alignments in a given experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c3ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import import_ipynb\n",
    "import system_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91986a9a",
   "metadata": {},
   "source": [
    "## Calculate Alignment Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50141f0",
   "metadata": {},
   "source": [
    "First we calculate the alignment errors of a given system on all evaluated measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01829ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAnnotationFile(annotfile):\n",
    "    '''\n",
    "    Parses a beat annotation file.\n",
    "    \n",
    "    Inputs\n",
    "    annotfile: filepath of the beat annotation file\n",
    "    \n",
    "    Returns a dictionary whose key is the measure number and whose value is the corresponding timestamp.\n",
    "    '''\n",
    "    df = pd.read_csv(annotfile, sep=',')\n",
    "    timestamps = np.array(df['start'])\n",
    "    measure_nums = np.array(df['measure'])\n",
    "    d = {}\n",
    "    for (t, m) in zip(timestamps, measure_nums):\n",
    "        d[m] = t\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc25aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEvalMeasureSet(scenarioInfo):\n",
    "    '''\n",
    "    Gets the list of measure indices at which to evaluate.\n",
    "    \n",
    "    Inputs\n",
    "    scenarioInfo: the scenario.info file for the scenario of interest\n",
    "    \n",
    "    Returns a sorted list of measure numbers.\n",
    "    '''\n",
    "    d = system_utils.get_scenario_info(scenarioInfo)\n",
    "    o_basename = os.path.splitext(os.path.basename(d['o']))[0] # e.g. rach2_mov1_O1\n",
    "    mov_id = '_'.join(o_basename.split('_')[0:2]) # e.g. rach2_mov1\n",
    "    measureSet = system_utils.get_eval_measure_set(mov_id)\n",
    "    return measureSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroundTruthTimestamps(annotfile1, annotfile2, scenarioInfo):\n",
    "    '''\n",
    "    Parses two beat annotation files and returns a list of the corresponding ground truth timestamps.\n",
    "    \n",
    "    Inputs\n",
    "    annotfile1: the first beat annotation file\n",
    "    annotfile2: the second beat annotation file\n",
    "    scenarioInfo: path to the scenario.info file\n",
    "    \n",
    "    Outputs\n",
    "    eval_pts: an Nx2 array of specifying the ground truth timestamps for N measures\n",
    "    overlap_measures: an array containing the list of evaluated measures, sorted in increasing order\n",
    "    '''\n",
    "    \n",
    "    # parse annotation files\n",
    "    gt1 = parseAnnotationFile(annotfile1)\n",
    "    gt2 = parseAnnotationFile(annotfile2)\n",
    "   \n",
    "    # measures to evaluate\n",
    "    allEvalMeasures = getEvalMeasureSet(scenarioInfo)\n",
    "    eval_measures = sorted(set(gt1).intersection(set(gt2)).intersection(allEvalMeasures))\n",
    "\n",
    "    # construct (t1, t2) ground truth timestamps\n",
    "    eval_pts = []\n",
    "    for m in eval_measures:\n",
    "        eval_pts.append((gt1[m], gt2[m]))\n",
    "    \n",
    "    return np.array(eval_pts), np.array(eval_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a28699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAlignErrors_single(hypfile, annotfile1, annotfile2, scenarioInfo):\n",
    "    '''\n",
    "    Calculates the alignment errors for a single hypothesis file.\n",
    "    \n",
    "    Inputs\n",
    "    hypfile: a .npy file containing the estimated alignment\n",
    "    annotfile1: the beat annotation file for the piano recording\n",
    "    annotfile2: the beat annotation file for the orchestra recording\n",
    "    scenarioInfo: path to the scenario.info file\n",
    "    \n",
    "    Outputs\n",
    "    err: the alignment errors in the estimated alignment\n",
    "    measNums: the measure numbers that are evaluated\n",
    "    '''\n",
    "    gt, measNums = getGroundTruthTimestamps(annotfile1, annotfile2, scenarioInfo) # ground truth\n",
    "    hypalign = np.load(hypfile) # piano-orchestra predicted alignment in sec\n",
    "    pred = np.interp(gt[:,0], hypalign[0,:], hypalign[1,:])\n",
    "    err = pred - gt[:,1]\n",
    "    return err, measNums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScenarioIds(scenarios_dir):\n",
    "    '''\n",
    "    Gets a list of scenario ids in a given scenarios/ directory.\n",
    "    \n",
    "    Inputs\n",
    "    scenarios_dir: directory containing scenarios information\n",
    "    \n",
    "    Returns a list of scenario ids, sorted in increasing order.\n",
    "    '''\n",
    "    summary_file = f'{scenarios_dir}/scenarios.summary'\n",
    "    d = system_utils.get_scenario_info(summary_file)\n",
    "    return list(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAlignErrors_batch(exp_dir, scenarios_dir, out_dir):\n",
    "    '''\n",
    "    Calculates the alignment errors for all scenarios in an experiment directory.\n",
    "    \n",
    "    Inputs\n",
    "    exp_dir: the experiment directory to evaluate\n",
    "    scenarios_dir: the directory containing the scenarios information\n",
    "    out_dir: the directory to save outputs and figures to\n",
    "    '''\n",
    "    # evaluate all scenarios\n",
    "    d = {}\n",
    "    for scenario_id in getScenarioIds(scenarios_dir):\n",
    "        hypFile = f'{exp_dir}/{scenario_id}/hyp.npy'\n",
    "        pianoAnnot = f'{scenarios_dir}/{scenario_id}/p.beats'\n",
    "        orchAnnot = f'{scenarios_dir}/{scenario_id}/o.beats'\n",
    "        scenarioInfo = f'{scenarios_dir}/{scenario_id}/scenario.info'\n",
    "        errs, measNums = calcAlignErrors_single(hypFile, pianoAnnot, orchAnnot, scenarioInfo)\n",
    "        d[scenario_id] = (errs, measNums) # key: scenario_id, value: (errors, measureNums)\n",
    "        \n",
    "    # save\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    outfile = f'{out_dir}/errs.pkl'\n",
    "    pickle.dump(d, open(outfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da968c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = 'experiments/match2' # change\n",
    "scenarios_dir = 'scenarios'\n",
    "eval_dir = 'eval/' + os.path.basename(exp_dir)\n",
    "calcAlignErrors_batch(exp_dir, scenarios_dir, eval_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341afa4",
   "metadata": {},
   "source": [
    "## Plot Error vs Tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484f7bcc",
   "metadata": {},
   "source": [
    "We can visualize the results by plotting the error rate across a range of error tolerances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotErrorVsTolerance(eval_dirs, maxTol, savefile = None):\n",
    "    '''\n",
    "    Plots the error rate across a range of error tolerances.\n",
    "    \n",
    "    Inputs\n",
    "    eval_dir: the eval directories to plot\n",
    "    maxTol: maximum error tolerance to consider (in milliseconds)\n",
    "    savefile: if specified, will save the figure to the given filepath\n",
    "    '''\n",
    "    \n",
    "    errRates_list = []\n",
    "    for eval_dir in eval_dirs:\n",
    "    \n",
    "        # load\n",
    "        with open(f'{eval_dir}/errs.pkl', 'rb') as f:\n",
    "            d = pickle.load(f)\n",
    "\n",
    "        # flattened list\n",
    "        errs = []\n",
    "        for scenario_id in d:\n",
    "            errs = np.append(errs, d[scenario_id][0])\n",
    "\n",
    "        # calculate error rates\n",
    "        errRates = np.zeros(maxTol+1)\n",
    "        tols = np.arange(maxTol+1)\n",
    "        for i in tols:\n",
    "            errRates[i] = np.mean(np.abs(errs) > i/1000)\n",
    "        errRates_list.append(errRates)\n",
    "            \n",
    "        # plot\n",
    "        plt.plot(tols, errRates * 100.0)\n",
    "        \n",
    "    plt.ylabel('Error Rate (%)')\n",
    "    plt.xlabel('Error Tolerance (ms)')\n",
    "    plt.legend([os.path.basename(eval_dir) for eval_dir in eval_dirs])\n",
    "    plt.grid(linestyle='--')\n",
    "    if savefile:\n",
    "        plt.savefig(savefile)\n",
    "\n",
    "    return errRates_list, tols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf87a9b",
   "metadata": {},
   "source": [
    "Plot the error rate vs error tolerance curve for one system of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTol = 5000 # in milliseconds\n",
    "eval_dir = 'eval/offlineDTW'\n",
    "errRates_list, tols = plotErrorVsTolerance([eval_dir], maxTol, savefile=False)\n",
    "errRates_list[0][1000]*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d180c93",
   "metadata": {},
   "source": [
    "Overlay multiple error curves for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "systems_to_compare = ['match','match2','offlineDTW']\n",
    "eval_dirs = [f'eval/{s}' for s in systems_to_compare]\n",
    "errRates_list, tols = plotErrorVsTolerance(eval_dirs, maxTol, savefile=False)\n",
    "[errRates_list[i][1000]*100.0 for i in range(len(eval_dirs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209e5f7",
   "metadata": {},
   "source": [
    "## Separate error curves by condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19717b",
   "metadata": {},
   "source": [
    "Visualize the same error curve for a single system, but separated by different conditions.  For example, one can visualize the performance across:\n",
    "- TSM factor\n",
    "- full mix recording\n",
    "- concerto\n",
    "- composer\n",
    "- chunk within a movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e3267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotErrorVsTolerance_separated(eval_dir, mapping, maxTol, savefile = None):\n",
    "    '''\n",
    "    Plots error rate across a range of error tolerances.  Data is separated into categories\n",
    "    specified in the given dictionary, and each category is plotted as a separate curve.\n",
    "    \n",
    "    Inputs\n",
    "    eval_dir: the eval directory to process\n",
    "    mapping: a dictionary whose key is the scenario id and whose value is the category name.\n",
    "      Any scenario ids that are not in the dictionary will be excluded from the plot.\n",
    "    maxTol: maximum error tolerance to consider (in milliseconds)\n",
    "    savefile: if specified, will save the figure to the given filepath\n",
    "    '''\n",
    "    \n",
    "    # initialize\n",
    "    categories = list(sorted(set(mapping.values())))\n",
    "    errors_by_category = {}\n",
    "    for c in categories:\n",
    "        errors_by_category[c] = [] # flattened list of alignment errors by category\n",
    "    \n",
    "    # load\n",
    "    with open(f'{eval_dir}/errs.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)  # key: scenario_id, value: (errors, measureNums)\n",
    "\n",
    "    # aggregate data by category\n",
    "    for scenario_id in d:\n",
    "        if scenario_id in mapping:\n",
    "            category = mapping[scenario_id]\n",
    "            errors_by_category[category] = np.append(errors_by_category[category], d[scenario_id][0])\n",
    "\n",
    "    # calculate error rates by category\n",
    "    errRates_list = {}\n",
    "    numPts = {}\n",
    "    for c in categories:\n",
    "        errRates = np.zeros(maxTol+1)\n",
    "        tols = np.arange(maxTol+1)\n",
    "        for i in tols:\n",
    "            errRates[i] = np.mean(np.abs(errors_by_category[c]) > i/1000)\n",
    "        errRates_list[c] = errRates\n",
    "        numPts[c] = len(errors_by_category[c]) # for debugging\n",
    "        plt.plot(tols, errRates * 100.0)\n",
    "        \n",
    "    plt.ylabel('Error Rate (%)')\n",
    "    plt.xlabel('Error Tolerance (ms)')\n",
    "    plt.legend(categories)\n",
    "    plt.grid(linestyle='--')\n",
    "    if savefile:\n",
    "        plt.savefig(savefile)\n",
    "\n",
    "    return errRates_list, tols, numPts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapByTSMFactor():\n",
    "    '''\n",
    "    Constructs a mapping separated by TSM factor.\n",
    "    '''\n",
    "    d = system_utils.get_scenario_info(SCENARIOS_SUMMARY)\n",
    "    mapping = {}\n",
    "    for scenario_id in d:\n",
    "        mapping[scenario_id] = d[scenario_id]['p'].split('/')[-2] # e.g. 'tsm0.80'\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d39993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapByFullMix():\n",
    "    '''\n",
    "    Constructs a mapping separated by full mix recording.\n",
    "    '''\n",
    "    d = system_utils.get_scenario_info(SCENARIOS_SUMMARY)\n",
    "    mapping = {}\n",
    "    for scenario_id in d:\n",
    "        mapping[scenario_id] = os.path.splitext(os.path.basename(d[scenario_id]['po']))[0] # e.g. 'rach2_mov1_PO1'\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapByChunk():\n",
    "    '''\n",
    "    Constructs a mapping separated by chunk within the movement.\n",
    "    '''\n",
    "    # construct mapping with tuple categories\n",
    "    d = system_utils.get_scenario_info(SCENARIOS_SUMMARY)\n",
    "    mapping = {}\n",
    "    for scenario_id in d:\n",
    "        mapping[scenario_id] = (d[scenario_id]['measStart'], d[scenario_id]['measEnd'])\n",
    "        \n",
    "    # map tuples to string (e.g. 'chunk1', 'chunk2'\n",
    "    tup2str = {}\n",
    "    for i, tup in enumerate(sorted(set(mapping.values()))):\n",
    "        tup2str[tup] = f'Chunk{i+1}'\n",
    "\n",
    "    # construct mapping with string categories\n",
    "    renamed = {}\n",
    "    for scenario_id in mapping:\n",
    "        renamed[scenario_id] = tup2str[mapping[scenario_id]]\n",
    "        \n",
    "    return renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = 'eval/offlineDTW'\n",
    "maxTol = 1000 # in milliseconds\n",
    "SCENARIOS_SUMMARY = 'scenarios/scenarios.summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de2fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates_list, tols, numPts = plotErrorVsTolerance_separated(eval_dir, mapByTSMFactor(), maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates_list, tols, numPts = plotErrorVsTolerance_separated(eval_dir, mapByFullMix(), maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b9053",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates_list, tols, numPts = plotErrorVsTolerance_separated(eval_dir, mapByChunk(), maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b72ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accompaniment",
   "language": "python",
   "name": "accompaniment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
