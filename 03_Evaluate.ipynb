{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0bb1e9",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6849e1b",
   "metadata": {},
   "source": [
    "This notebook evaluates the quality of the online alignments in a given experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c3ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e8c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# from pathlib import Path\n",
    "# import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91986a9a",
   "metadata": {},
   "source": [
    "## Calculate Alignment Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50141f0",
   "metadata": {},
   "source": [
    "First we calculate the alignment errors of a given system on all evaluated measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01829ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseAnnotationFile(annotfile):\n",
    "    '''\n",
    "    Parses a beat annotation file.\n",
    "    \n",
    "    Inputs\n",
    "    annotfile: filepath of the beat annotation file\n",
    "    \n",
    "    Returns a dictionary whose key is the measure number and whose value is the corresponding timestamp.\n",
    "    '''\n",
    "    df = pd.read_csv(annotfile, sep=',')\n",
    "    timestamps = np.array(df['start'])\n",
    "    measure_nums = np.array(df['measure'])\n",
    "    d = {}\n",
    "    for (t, m) in zip(timestamps, measure_nums):\n",
    "        d[m] = t\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGroundTruthTimestamps(annotfile1, annotfile2):\n",
    "    '''\n",
    "    Parses two beat annotation files and returns a list of the corresponding ground truth timestamps.\n",
    "    \n",
    "    Inputs\n",
    "    annotfile1: the first beat annotation file\n",
    "    annotfile2: the second beat annotation file\n",
    "    \n",
    "    Outputs\n",
    "    eval_pts: an Nx2 array of specifying the ground truth timestamps for N measures\n",
    "    overlap_measures: an array containing the list of evaluated measures, sorted in increasing order\n",
    "    '''\n",
    "    \n",
    "    # parse annotation files\n",
    "    gt1 = parseAnnotationFile(annotfile1)\n",
    "    gt2 = parseAnnotationFile(annotfile2)\n",
    "\n",
    "    # determine which measures to evaluate\n",
    "    overlap_measures = sorted(set(gt1).intersection(set(gt2)))\n",
    "    \n",
    "    # construct (t1, t2) ground truth timestamps\n",
    "    eval_pts = []\n",
    "    for m in overlap_measures:\n",
    "        eval_pts.append((gt1[m], gt2[m]))\n",
    "    \n",
    "    return np.array(eval_pts), np.array(overlap_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a28699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAlignErrors_single(hypfile, annotfile1, annotfile2, hop_sec):\n",
    "    '''\n",
    "    Calculates the alignment errors for a single hypothesis file.\n",
    "    \n",
    "    Inputs\n",
    "    hypfile: a .npy file containing the estimated alignment\n",
    "    annotfile1: the beat annotation file for the piano recording\n",
    "    annotfile2: the beat annotation file for the orchestra recording\n",
    "    hop_sec: hop size between frames in the DTW alignment\n",
    "    \n",
    "    Outputs\n",
    "    err: the alignment errors in the estimated alignment\n",
    "    measNums: the measure numbers that are evaluated\n",
    "    '''\n",
    "    gt, measNums = getGroundTruthTimestamps(annotfile1, annotfile2) # ground truth\n",
    "    hypalign = np.load(hypfile) # piano-orchestra predicted alignment in frames\n",
    "    pred = np.interp(gt[:,0], hypalign[0,:]*hop_sec, hypalign[1,:]*hop_sec)\n",
    "    err = pred - gt[:,1]\n",
    "    return err, measNums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScenarioIds(scenarios_dir):\n",
    "    '''\n",
    "    Gets a list of scenario ids in a given scenarios/ directory.\n",
    "    \n",
    "    Inputs\n",
    "    scenarios_dir: directory containing scenarios information\n",
    "    \n",
    "    Returns a list of scenario ids, sorted in increasing order.\n",
    "    '''\n",
    "    d = pd.read_csv(f'{scenarios_dir}/scenarios.summary', header=None, sep=' ')\n",
    "    return list(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d9b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAlignErrors_batch(exp_dir, scenarios_dir, hop_sec, out_dir):\n",
    "    '''\n",
    "    Calculates the alignment errors for all scenarios in an experiment directory.\n",
    "    \n",
    "    Inputs\n",
    "    exp_dir: the experiment directory to evaluate\n",
    "    scenarios_dir: the directory containing the scenarios information\n",
    "    hop_sec: hop size between frames in the DTW alignment\n",
    "    out_dir: the directory to save outputs and figures to\n",
    "    '''\n",
    "    # evaluate all scenarios\n",
    "    d = {} \n",
    "    for scenario_id in getScenarioIds(scenarios_dir):\n",
    "        hypFile = f'{exp_dir}/{scenario_id}/hyp.npy'\n",
    "        pianoAnnot = f'{scenarios_dir}/{scenario_id}/p.beats'\n",
    "        orchAnnot = f'{scenarios_dir}/{scenario_id}/o.beats'\n",
    "        err, measNums = calcAlignErrors_single(hypFile, pianoAnnot, orchAnnot, hop_sec)\n",
    "        d[scenario_id] = (err, measNums) # key: scenario_id, value: (errors, measureNums)\n",
    "        \n",
    "    # save\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    outfile = f'{out_dir}/errs.pkl'\n",
    "    pickle.dump(d, open(outfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da968c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = 'experiments/simpleOfflineDTW' # change\n",
    "scenarios_dir = 'scenarios'\n",
    "eval_dir = 'eval/' + os.path.basename(exp_dir)\n",
    "hop_sec = 512./22050\n",
    "calcAlignErrors_batch(exp_dir, scenarios_dir, hop_sec, eval_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341afa4",
   "metadata": {},
   "source": [
    "## Plot Error vs Tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484f7bcc",
   "metadata": {},
   "source": [
    "We can visualize the results by plotting the error rate across a range of error tolerances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotErrorVsTolerance(eval_dir, maxTol, savepng = None):\n",
    "    '''\n",
    "    Plots the error rate across a range of error tolerances.\n",
    "    \n",
    "    Inputs\n",
    "    eval_dir: the eval directory to plot\n",
    "    maxTol: maximum error tolerance to consider (in milliseconds)\n",
    "    savepng: if True, will save the figure to file as png image\n",
    "    '''\n",
    "    # load\n",
    "    with open(f'{eval_dir}/errs.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    \n",
    "    # flattened list\n",
    "    errs = []\n",
    "    for scenario_id in d:\n",
    "        errs = np.append(errs, d[scenario_id][0])\n",
    "\n",
    "    # calculate error rates\n",
    "    errRates = np.zeros(maxTol+1)\n",
    "    tols = np.arange(maxTol+1)\n",
    "    for i in tols:\n",
    "        errRates[i] = np.mean(np.abs(errs) > i/1000)\n",
    "        \n",
    "    # plot\n",
    "    plt.plot(tols, errRates * 100.0)\n",
    "    plt.ylabel('Error Rate (%)')\n",
    "    plt.xlabel('Error Tolerance (ms)')\n",
    "    if savepng:\n",
    "        plt.savefig(f'{eval_dir}/errorVsTol.png')\n",
    "    \n",
    "    return errRates, tols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTol = 5000 # in milliseconds\n",
    "errRates, tols = plotErrorVsTolerance(eval_dir, maxTol, savepng=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f0e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "errRates[1000]*100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113deb60",
   "metadata": {},
   "source": [
    "## Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9113485",
   "metadata": {},
   "source": [
    "Below are some analysis plots to gain intuition in system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77eac28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{eval_dir}/errs.pkl', 'rb') as f:\n",
    "    d = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_id = 's12'\n",
    "errs = d[scenario_id][0]\n",
    "measures = len(errs)\n",
    "plt.plot(np.arange(measures), errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ebb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(len(errs)), errs)\n",
    "# plt.xlabel('Measure Number')\n",
    "# plt.ylabel('Alignment Error (sec)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accompaniment",
   "language": "python",
   "name": "accompaniment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
